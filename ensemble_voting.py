"""
çŒ«ç‹—åˆ†ç±» - é›†æˆå­¦ä¹ æŠ•ç¥¨é¢„æµ‹è„šæœ¬

æœ¬è„šæœ¬åŠ è½½å·²è®­ç»ƒçš„ SVMã€é€»è¾‘å›å½’ã€éšæœºæ£®æ— ä¸‰ä¸ªæ¨¡å‹ï¼Œ
ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶è¿›è¡Œé›†æˆé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æœã€‚

æ”¯æŒä¸¤ç§æŠ•ç¥¨æ–¹å¼ï¼š
    - ç¡¬æŠ•ç¥¨ï¼ˆHard Votingï¼‰ï¼šåŸºäºé¢„æµ‹ç±»åˆ«è¿›è¡Œå¤šæ•°æŠ•ç¥¨
    - è½¯æŠ•ç¥¨ï¼ˆSoft Votingï¼‰ï¼šåŸºäºé¢„æµ‹æ¦‚ç‡è¿›è¡ŒåŠ æƒå¹³å‡
"""

import argparse
import time
import joblib
import numpy as np
from pathlib import Path
from datetime import datetime

from src.utils.ml_training import (
    load_train_val_test,
    compute_classification_metrics,
)
from src.utils.logger import Logger
from tools.visualization import (
    plot_split_metrics,
    plot_confusion_matrix,
    plot_roc_curve,
)
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression


class VotingEnsemble:
    """æŠ•ç¥¨é›†æˆåˆ†ç±»å™¨
    
    å°è£…äº†å¤šä¸ªæ¨¡å‹çš„æŠ•ç¥¨é¢„æµ‹åŠŸèƒ½ï¼Œæ”¯æŒç¡¬æŠ•ç¥¨å’Œè½¯æŠ•ç¥¨ä¸¤ç§æ–¹å¼ã€‚
    
    å‚æ•°:
        models: æ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªæ¨¡å‹å¿…é¡»å®ç° predict() å’Œ predict_proba() æ–¹æ³•
        model_names: æ¨¡å‹åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        voting: æŠ•ç¥¨æ–¹å¼ï¼Œ'hard' æˆ– 'soft'ï¼ˆé»˜è®¤: 'soft'ï¼‰
        weights: æ¨¡å‹æƒé‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤ç­‰æƒé‡ï¼‰
    """
    
    def __init__(self, models, model_names=None, voting='soft', weights=None):
        if not models:
            raise ValueError("æ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        self.models = models
        self.model_names = model_names or [f"Model_{i+1}" for i in range(len(models))]
        if len(self.model_names) != len(models):
            raise ValueError("æ¨¡å‹åç§°æ•°é‡å¿…é¡»ä¸æ¨¡å‹æ•°é‡ä¸€è‡´")
        
        self.voting = voting.lower()
        if self.voting not in ['hard', 'soft']:
            raise ValueError("voting å¿…é¡»æ˜¯ 'hard' æˆ– 'soft'")
        
        if weights is None:
            self.weights = [1.0] * len(models)
        else:
            if len(weights) != len(models):
                raise ValueError("æƒé‡æ•°é‡å¿…é¡»ä¸æ¨¡å‹æ•°é‡ä¸€è‡´")
            self.weights = np.array(weights, dtype=float)
            self.weights = self.weights / self.weights.sum()  # å½’ä¸€åŒ–
        
        self.logger = None
    
    def _get_logger(self):
        """è·å–loggerï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„"""
        if self.logger is None:
            self.logger = Logger(name="voting_ensemble")
        return self.logger
    
    def predict(self, X):
        """è¿›è¡Œç¡¬æŠ•ç¥¨é¢„æµ‹
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_samples, n_features)
            
        è¿”å›:
            é¢„æµ‹æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n_samples,)
        """
        if self.voting == 'hard':
            predictions = np.array([model.predict(X) for model in self.models])
            # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡ŒåŠ æƒæŠ•ç¥¨
            weighted_votes = np.zeros((X.shape[0], 2))
            for i, (pred, weight) in enumerate(zip(predictions, self.weights)):
                for j, label in enumerate(pred):
                    weighted_votes[j, int(label)] += weight
            return np.argmax(weighted_votes, axis=1)
        else:
            # è½¯æŠ•ç¥¨ï¼šä½¿ç”¨æ¦‚ç‡çš„å¹³å‡å€¼
            return self.predict_proba(X).argmax(axis=1)
    
    def predict_proba(self, X):
        """è¿›è¡Œè½¯æŠ•ç¥¨é¢„æµ‹ï¼ˆè¿”å›æ¦‚ç‡ï¼‰
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_samples, n_features)
            
        è¿”å›:
            é¢„æµ‹æ¦‚ç‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n_samples, n_classes)
        """
        if self.voting == 'hard':
            # ç¡¬æŠ•ç¥¨æ¨¡å¼ä¸‹ï¼Œå°†é¢„æµ‹è½¬æ¢ä¸ºæ¦‚ç‡
            predictions = np.array([model.predict(X) for model in self.models])
            proba = np.zeros((X.shape[0], 2))
            for i, (pred, weight) in enumerate(zip(predictions, self.weights)):
                for j, label in enumerate(pred):
                    proba[j, int(label)] += weight
            return proba / proba.sum(axis=1, keepdims=True)
        else:
            # è½¯æŠ•ç¥¨ï¼šåŠ æƒå¹³å‡æ¦‚ç‡
            probas = []
            for model, weight in zip(self.models, self.weights):
                try:
                    proba = model.predict_proba(X)
                    if proba.ndim == 2 and proba.shape[1] == 2:
                        probas.append(proba * weight)
                    else:
                        # å¦‚æœåªæœ‰ä¸€ç»´ï¼Œè½¬æ¢ä¸ºäºŒç»´
                        if proba.ndim == 1:
                            proba_2d = np.column_stack([1 - proba, proba])
                        else:
                            proba_2d = proba
                        probas.append(proba_2d * weight)
                except AttributeError:
                    # å¦‚æœæ²¡æœ‰ predict_probaï¼Œä½¿ç”¨ decision_function
                    try:
                        scores = model.decision_function(X)
                        # å°†å†³ç­–å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç®€å• sigmoidï¼‰
                        proba = 1 / (1 + np.exp(-scores))
                        proba_2d = np.column_stack([1 - proba, proba])
                        probas.append(proba_2d * weight)
                    except AttributeError:
                        # æœ€åå°è¯•ï¼šä½¿ç”¨ç¡¬é¢„æµ‹
                        pred = model.predict(X)
                        proba = np.zeros((X.shape[0], 2))
                        for j, label in enumerate(pred):
                            proba[j, int(label)] = 1.0
                        probas.append(proba * weight)
            
            if not probas:
                raise RuntimeError("æ— æ³•ä»ä»»ä½•æ¨¡å‹è·å–æ¦‚ç‡é¢„æµ‹")
            
            ensemble_proba = np.sum(probas, axis=0)
            # å½’ä¸€åŒ–
            row_sums = ensemble_proba.sum(axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1
            ensemble_proba = ensemble_proba / row_sums
            return ensemble_proba
    
    def evaluate(self, X, y, name):
        """åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°é›†æˆæ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_samples, n_features)
            y: çœŸå®æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n_samples,)
            name: æ•°æ®é›†åç§°ï¼ˆå¦‚ "Validation", "Test"ï¼‰ï¼Œç”¨äºæ‰“å°ä¿¡æ¯
            
        è¿”å›:
            dict: ä¸»è¦åˆ†ç±»æŒ‡æ ‡ï¼ˆaccuracy/precision/recall/f1/aucï¼‰
        """
        logger = self._get_logger()
        
        # è¿›è¡Œé¢„æµ‹
        y_pred = self.predict(X)
        y_pred_proba = self.predict_proba(X)[:, 1]
        
        metrics_dict = compute_classification_metrics(
            y, y_pred, y_proba=y_pred_proba, positive_label=1
        )
        
        parts = [
            "acc={:.4f}".format(metrics_dict['accuracy']),
            "prec={:.4f}".format(metrics_dict['precision']),
            "rec={:.4f}".format(metrics_dict['recall']),
            "f1={:.4f}".format(metrics_dict['f1']),
        ]
        if 'auc' in metrics_dict:
            parts.append("auc={:.4f}".format(metrics_dict['auc']))
        
        msg = "{} | {}".format(name, ", ".join(parts))
        logger.info(msg)
        return metrics_dict


class StackingEnsemble:
    """Stacking é›†æˆåˆ†ç±»å™¨
    
    ä½¿ç”¨å…ƒå­¦ä¹ å™¨å­¦ä¹ å¦‚ä½•ç»„åˆåŸºå­¦ä¹ å™¨çš„é¢„æµ‹ã€‚
    é€šè¿‡äº¤å‰éªŒè¯ç”Ÿæˆ out-of-fold é¢„æµ‹ï¼Œé¿å…æ•°æ®æ³„éœ²ã€‚
    
    å‚æ•°:
        base_models: åŸºæ¨¡å‹åˆ—è¡¨
        base_model_names: åŸºæ¨¡å‹åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        meta_model: å…ƒå­¦ä¹ å™¨ï¼ˆé»˜è®¤: LogisticRegressionï¼‰
        cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆé»˜è®¤: 5ï¼‰
        use_proba: æ˜¯å¦ä½¿ç”¨æ¦‚ç‡ä½œä¸ºç‰¹å¾ï¼ˆé»˜è®¤: Trueï¼‰
        random_state: éšæœºç§å­
    """
    
    def __init__(self, base_models, base_model_names=None, meta_model=None, 
                 cv_folds=5, use_proba=True, random_state=42):
        if not base_models:
            raise ValueError("åŸºæ¨¡å‹åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        self.base_models = base_models
        self.base_model_names = base_model_names or [f"Base_{i+1}" for i in range(len(base_models))]
        if len(self.base_model_names) != len(base_models):
            raise ValueError("åŸºæ¨¡å‹åç§°æ•°é‡å¿…é¡»ä¸æ¨¡å‹æ•°é‡ä¸€è‡´")
        
        self.cv_folds = cv_folds
        self.use_proba = use_proba
        self.random_state = random_state
        
        # é»˜è®¤ä½¿ç”¨é€»è¾‘å›å½’ä½œä¸ºå…ƒå­¦ä¹ å™¨
        if meta_model is None:
            self.meta_model = LogisticRegression(
                max_iter=1000,
                random_state=random_state,
                solver='liblinear'
            )
        else:
            self.meta_model = meta_model
        
        self.trained_base_models = None
        self.logger = None
    
    def _get_logger(self):
        """è·å–loggerï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„"""
        if self.logger is None:
            self.logger = Logger(name="stacking_ensemble")
        return self.logger
    
    def _get_base_predictions(self, model, X, use_proba=True):
        """è·å–åŸºæ¨¡å‹çš„é¢„æµ‹
        
        å‚æ•°:
            model: åŸºæ¨¡å‹
            X: ç‰¹å¾çŸ©é˜µ
            use_proba: æ˜¯å¦ä½¿ç”¨æ¦‚ç‡
            
        è¿”å›:
            é¢„æµ‹ç‰¹å¾ï¼ˆæ¦‚ç‡æˆ–ç±»åˆ«ï¼‰
        """
        if use_proba:
            try:
                proba = model.predict_proba(X)
                if proba.ndim == 2 and proba.shape[1] == 2:
                    return proba[:, 1]  # åªè¿”å›æ­£ç±»æ¦‚ç‡
                else:
                    return proba.flatten()
            except AttributeError:
                # å¦‚æœæ²¡æœ‰ predict_probaï¼Œä½¿ç”¨ decision_function
                try:
                    scores = model.decision_function(X)
                    # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆsigmoidï¼‰
                    proba = 1 / (1 + np.exp(-np.clip(scores, -500, 500)))
                    return proba
                except AttributeError:
                    # æœ€åä½¿ç”¨ç¡¬é¢„æµ‹
                    pred = model.predict(X)
                    return pred.astype(float)
        else:
            return model.predict(X).astype(float)
    
    def fit(self, X_train, y_train):
        """è®­ç»ƒ Stacking é›†æˆæ¨¡å‹
        
        å‚æ•°:
            X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µ
            y_train: è®­ç»ƒæ ‡ç­¾æ•°ç»„
        """
        logger = self._get_logger()
        logger.info("ğŸ”§ å¼€å§‹è®­ç»ƒ Stacking é›†æˆæ¨¡å‹...")
        logger.info(f"   åŸºæ¨¡å‹æ•°é‡: {len(self.base_models)}")
        logger.info(f"   äº¤å‰éªŒè¯æŠ˜æ•°: {self.cv_folds}")
        logger.info(f"   ä½¿ç”¨æ¦‚ç‡ç‰¹å¾: {self.use_proba}")
        
        n_samples = X_train.shape[0]
        n_base_models = len(self.base_models)
        
        # åˆå§‹åŒ– out-of-fold é¢„æµ‹çŸ©é˜µ
        if self.use_proba:
            oof_predictions = np.zeros((n_samples, n_base_models))
        else:
            oof_predictions = np.zeros((n_samples, n_base_models))
        
        # å­˜å‚¨è®­ç»ƒå¥½çš„åŸºæ¨¡å‹ï¼ˆç”¨äºæµ‹è¯•æ—¶é¢„æµ‹ï¼‰
        self.trained_base_models = []
        
        # äº¤å‰éªŒè¯ç”Ÿæˆ out-of-fold é¢„æµ‹
        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
            y_fold_train = y_train[train_idx]
            
            logger.info(f"   è®­ç»ƒ Fold {fold_idx + 1}/{self.cv_folds}...")
            
            # åœ¨æ¯ä¸ª fold ä¸Šè®­ç»ƒåŸºæ¨¡å‹
            for model_idx, (base_model, model_name) in enumerate(zip(self.base_models, self.base_model_names)):
                # å…‹éš†æ¨¡å‹ï¼ˆé¿å…ä¿®æ”¹åŸå§‹æ¨¡å‹ï¼‰
                from sklearn.base import clone
                model_clone = clone(base_model)
                
                # è®­ç»ƒæ¨¡å‹
                model_clone.fit(X_fold_train, y_fold_train)
                
                # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
                val_pred = self._get_base_predictions(model_clone, X_fold_val, self.use_proba)
                oof_predictions[val_idx, model_idx] = val_pred
        
        logger.info("âœ… åŸºæ¨¡å‹äº¤å‰éªŒè¯å®Œæˆ")
        
        # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒæ‰€æœ‰åŸºæ¨¡å‹ï¼ˆç”¨äºæµ‹è¯•æ—¶é¢„æµ‹ï¼‰
        logger.info("ğŸ”„ åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒåŸºæ¨¡å‹...")
        for model_idx, (base_model, model_name) in enumerate(zip(self.base_models, self.base_model_names)):
            from sklearn.base import clone
            model_clone = clone(base_model)
            model_clone.fit(X_train, y_train)
            self.trained_base_models.append(model_clone)
            logger.info(f"   âœ… {model_name} è®­ç»ƒå®Œæˆ")
        
        # è®­ç»ƒå…ƒå­¦ä¹ å™¨
        logger.info("ğŸ¯ è®­ç»ƒå…ƒå­¦ä¹ å™¨...")
        self.meta_model.fit(oof_predictions, y_train)
        logger.info("âœ… Stacking é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # æ˜¾ç¤ºå…ƒå­¦ä¹ å™¨ç³»æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self.meta_model, 'coef_'):
            coef = self.meta_model.coef_[0]
            logger.info("ğŸ“Š å…ƒå­¦ä¹ å™¨ç³»æ•°:")
            for name, c in zip(self.base_model_names, coef):
                logger.info(f"   {name}: {c:.4f}")
    
    def predict(self, X):
        """è¿›è¡Œé¢„æµ‹
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µ
            
        è¿”å›:
            é¢„æµ‹æ ‡ç­¾æ•°ç»„
        """
        if self.trained_base_models is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        # è·å–æ‰€æœ‰åŸºæ¨¡å‹çš„é¢„æµ‹
        base_preds = []
        for model in self.trained_base_models:
            pred = self._get_base_predictions(model, X, self.use_proba)
            base_preds.append(pred)
        
        # ç»„åˆæˆç‰¹å¾çŸ©é˜µ
        meta_features = np.column_stack(base_preds)
        
        # ä½¿ç”¨å…ƒå­¦ä¹ å™¨é¢„æµ‹
        return self.meta_model.predict(meta_features)
    
    def predict_proba(self, X):
        """è¿”å›é¢„æµ‹æ¦‚ç‡
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µ
            
        è¿”å›:
            é¢„æµ‹æ¦‚ç‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n_samples, n_classes)
        """
        if self.trained_base_models is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ fit() æ–¹æ³•")
        
        # è·å–æ‰€æœ‰åŸºæ¨¡å‹çš„é¢„æµ‹
        base_preds = []
        for model in self.trained_base_models:
            pred = self._get_base_predictions(model, X, self.use_proba)
            base_preds.append(pred)
        
        # ç»„åˆæˆç‰¹å¾çŸ©é˜µ
        meta_features = np.column_stack(base_preds)
        
        # ä½¿ç”¨å…ƒå­¦ä¹ å™¨é¢„æµ‹æ¦‚ç‡
        return self.meta_model.predict_proba(meta_features)
    
    def evaluate(self, X, y, name):
        """åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µ
            y: çœŸå®æ ‡ç­¾æ•°ç»„
            name: æ•°æ®é›†åç§°
            
        è¿”å›:
            dict: ä¸»è¦åˆ†ç±»æŒ‡æ ‡
        """
        logger = self._get_logger()
        
        y_pred = self.predict(X)
        y_pred_proba = self.predict_proba(X)[:, 1]
        
        metrics_dict = compute_classification_metrics(
            y, y_pred, y_proba=y_pred_proba, positive_label=1
        )
        
        parts = [
            "acc={:.4f}".format(metrics_dict['accuracy']),
            "prec={:.4f}".format(metrics_dict['precision']),
            "rec={:.4f}".format(metrics_dict['recall']),
            "f1={:.4f}".format(metrics_dict['f1']),
        ]
        if 'auc' in metrics_dict:
            parts.append("auc={:.4f}".format(metrics_dict['auc']))
        
        msg = "{} | {}".format(name, ", ".join(parts))
        logger.info(msg)
        return metrics_dict


def load_model(model_path, model_name="Model"):
    """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
    
    å‚æ•°:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆ.joblib æ–‡ä»¶ï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        
    è¿”å›:
        åŠ è½½çš„æ¨¡å‹å¯¹è±¡
    """
    model_path = Path(model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    try:
        model = joblib.load(model_path)
        Logger(name="ensemble").info(f"âœ… æˆåŠŸåŠ è½½ {model_name}: {model_path}")
        return model
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥ {model_path}: {e}")


def build_parser():
    """æ„å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="é›†æˆå­¦ä¹ æŠ•ç¥¨é¢„æµ‹è„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    
    # æ¨¡å‹è·¯å¾„å‚æ•°
    parser.add_argument(
        '--lr-model', type=str,
        default='runs/sklearn_lr_20251112-195315/best.joblib',
        help='é€»è¾‘å›å½’æ¨¡å‹è·¯å¾„'
    )
    parser.add_argument(
        '--rf-model', type=str,
        default='runs/sklearn_rf_20251113-085457/best.joblib',
        help='éšæœºæ£®æ—æ¨¡å‹è·¯å¾„'
    )
    parser.add_argument(
        '--svm-model', type=str,
        default='runs/sklearn_svm_20251112-195558/best.joblib',
        help='SVM æ¨¡å‹è·¯å¾„'
    )
    
    # æ•°æ®å‚æ•°
    parser.add_argument(
        '--data-dir', type=str, default='features',
        help='ç‰¹å¾æ–‡ä»¶ç›®å½•'
    )
    parser.add_argument(
        '--train-dirname', type=str, default='train',
        help='è®­ç»ƒé›†å­ç›®å½•å'
    )
    parser.add_argument(
        '--val-dirname', type=str, default='val',
        help='éªŒè¯é›†å­ç›®å½•å'
    )
    parser.add_argument(
        '--test-dirname', type=str, default='test',
        help='æµ‹è¯•é›†å­ç›®å½•å'
    )
    
    # æŠ•ç¥¨å‚æ•°
    parser.add_argument(
        '--method', type=str, choices=['voting', 'stacking'],
        default='voting', help='é›†æˆæ–¹æ³•ï¼švotingï¼ˆæŠ•ç¥¨ï¼‰æˆ– stackingï¼ˆå †å ï¼‰'
    )
    parser.add_argument(
        '--voting', type=str, choices=['hard', 'soft'],
        default='soft', help='æŠ•ç¥¨æ–¹å¼ï¼šhardï¼ˆç¡¬æŠ•ç¥¨ï¼‰æˆ– softï¼ˆè½¯æŠ•ç¥¨ï¼‰ï¼ˆä»…å½“ method=voting æ—¶æœ‰æ•ˆï¼‰'
    )
    parser.add_argument(
        '--weights', type=float, nargs='+', default=None,
        help='æ¨¡å‹æƒé‡åˆ—è¡¨ï¼ˆé¡ºåºï¼šLR, RF, SVMï¼‰ï¼Œé»˜è®¤ç­‰æƒé‡'
    )
    parser.add_argument(
        '--auto-weights', action='store_true',
        help='æ ¹æ®éªŒè¯é›†æ€§èƒ½è‡ªåŠ¨è®¡ç®—æƒé‡ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰'
    )
    parser.add_argument(
        '--weight-power', type=float, default=2.0,
        help='æƒé‡è®¡ç®—çš„å¹‚æ¬¡ï¼ˆé»˜è®¤: 2.0ï¼Œè¶Šå¤§åˆ™å¥½æ¨¡å‹æƒé‡è¶Šé«˜ï¼‰'
    )
    parser.add_argument(
        '--top-k', type=int, default=None,
        help='åªä½¿ç”¨è¡¨ç°æœ€å¥½çš„Kä¸ªæ¨¡å‹ï¼ˆé»˜è®¤: ä½¿ç”¨æ‰€æœ‰æ¨¡å‹ï¼‰'
    )
    parser.add_argument(
        '--min-accuracy', type=float, default=None,
        help='æœ€å°å‡†ç¡®ç‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ¨¡å‹å°†è¢«æ’é™¤ï¼ˆé»˜è®¤: ä¸æ’é™¤ï¼‰'
    )
    parser.add_argument(
        '--analyze', action='store_true',
        help='è¿›è¡Œè¯¦ç»†çš„æ¨¡å‹åˆ†æï¼ˆé¢„æµ‹ä¸€è‡´æ€§ã€é”™è¯¯åˆ†æç­‰ï¼‰'
    )
    parser.add_argument(
        '--stacking-cv', type=int, default=5,
        help='Stacking äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆé»˜è®¤: 5ï¼‰'
    )
    parser.add_argument(
        '--stacking-use-proba', type=lambda x: (str(x).lower() == 'true'),
        default=True, metavar='BOOL',
        help='Stacking æ˜¯å¦ä½¿ç”¨æ¦‚ç‡ä½œä¸ºç‰¹å¾ï¼ˆé»˜è®¤: Trueï¼‰'
    )
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument(
        '--save-dir', type=str, default=None,
        help='ç»“æœä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ï¼šruns/ensemble_{timestamp}ï¼‰'
    )
    parser.add_argument(
        '--log-level', type=str, default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='æ—¥å¿—çº§åˆ«'
    )
    
    return parser


def analyze_models(models, model_names, datasets, save_dir, logger):
    """åˆ†ææ¨¡å‹ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§ã€é”™è¯¯æ¨¡å¼ç­‰
    
    å‚æ•°:
        models: æ¨¡å‹åˆ—è¡¨
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        datasets: æ•°æ®é›†å­—å…¸
        save_dir: ä¿å­˜ç›®å½•
        logger: Logger å¯¹è±¡
    """
    X_val, y_val = datasets.get('val', (None, None))
    if X_val is None or y_val is None:
        logger.warning("âš ï¸  éªŒè¯é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡è¯¦ç»†åˆ†æ")
        return
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æ¨¡å‹è¯Šæ–­åˆ†ææŠ¥å‘Š")
    logger.info("="*60)
    
    # 1. è·å–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
    all_predictions = []
    all_probas = []
    
    for model, name in zip(models, model_names):
        try:
            y_pred = model.predict(X_val)
            all_predictions.append(y_pred)
            
            # è·å–æ¦‚ç‡
            try:
                proba = model.predict_proba(X_val)
                if proba.ndim == 2 and proba.shape[1] == 2:
                    all_probas.append(proba[:, 1])
                else:
                    all_probas.append(proba)
            except:
                all_probas.append(None)
        except Exception as e:
            logger.warning(f"   {name}: é¢„æµ‹å¤±è´¥ - {e}")
            all_predictions.append(None)
            all_probas.append(None)
    
    all_predictions = np.array([p for p in all_predictions if p is not None])
    if len(all_predictions) == 0:
        logger.warning("âš ï¸  æ— æ³•è·å–ä»»ä½•æ¨¡å‹çš„é¢„æµ‹ï¼Œè·³è¿‡åˆ†æ")
        return
    
    # 2. è®¡ç®—æ¨¡å‹é—´çš„ä¸€è‡´æ€§
    logger.info("\nğŸ“ˆ æ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§åˆ†æ:")
    n_models = len(all_predictions)
    n_samples = len(X_val)
    
    # è®¡ç®—æ¯å¯¹æ¨¡å‹ä¹‹é—´çš„ä¸€è‡´æ€§
    from sklearn.metrics import cohen_kappa_score
    agreement_matrix = np.zeros((n_models, n_models))
    for i in range(n_models):
        for j in range(i+1, n_models):
            kappa = cohen_kappa_score(all_predictions[i], all_predictions[j])
            agreement_matrix[i, j] = kappa
            agreement_matrix[j, i] = kappa
            logger.info(f"   {model_names[i]} vs {model_names[j]}: Kappa = {kappa:.4f}")
    
    # 3. åˆ†æå®Œå…¨ä¸€è‡´çš„æ ·æœ¬
    all_agree = np.all(all_predictions == all_predictions[0], axis=0)
    n_agree = np.sum(all_agree)
    logger.info(f"\nâœ… æ‰€æœ‰æ¨¡å‹é¢„æµ‹ä¸€è‡´çš„æ ·æœ¬: {n_agree}/{n_samples} ({n_agree/n_samples*100:.2f}%)")
    
    # 4. åˆ†æåˆ†æ­§æ ·æœ¬
    disagreements = ~all_agree
    n_disagree = np.sum(disagreements)
    logger.info(f"âŒ æ¨¡å‹é¢„æµ‹å­˜åœ¨åˆ†æ­§çš„æ ·æœ¬: {n_disagree}/{n_samples} ({n_disagree/n_samples*100:.2f}%)")
    
    # 5. åˆ†ææ¯ä¸ªæ¨¡å‹çš„é”™è¯¯
    logger.info("\nğŸ” å„æ¨¡å‹é”™è¯¯åˆ†æ:")
    from sklearn.metrics import accuracy_score, confusion_matrix
    model_errors = {}
    for i, (pred, name) in enumerate(zip(all_predictions, model_names)):
        acc = accuracy_score(y_val, pred)
        errors = (pred != y_val)
        model_errors[name] = {
            'accuracy': acc,
            'error_mask': errors,
            'n_errors': np.sum(errors)
        }
        logger.info(f"   {name}: å‡†ç¡®ç‡ = {acc:.4f}, é”™è¯¯æ•° = {np.sum(errors)}")
    
    # 6. åˆ†æé”™è¯¯æ ·æœ¬çš„é‡å 
    logger.info("\nğŸ”— æ¨¡å‹é”™è¯¯é‡å åˆ†æ:")
    error_overlap = {}
    for i, name1 in enumerate(model_names):
        for j, name2 in enumerate(model_names):
            if i < j:
                errors1 = model_errors[name1]['error_mask']
                errors2 = model_errors[name2]['error_mask']
                both_wrong = np.sum(errors1 & errors2)
                only1_wrong = np.sum(errors1 & ~errors2)
                only2_wrong = np.sum(~errors1 & errors2)
                neither_wrong = np.sum(~errors1 & ~errors2)
                
                overlap_ratio = both_wrong / (both_wrong + only1_wrong + only2_wrong) if (both_wrong + only1_wrong + only2_wrong) > 0 else 0
                logger.info(f"   {name1} vs {name2}:")
                logger.info(f"      å…±åŒé”™è¯¯: {both_wrong}, ä»…{name1}é”™è¯¯: {only1_wrong}, ä»…{name2}é”™è¯¯: {only2_wrong}")
                logger.info(f"      é”™è¯¯é‡å ç‡: {overlap_ratio:.4f}")
                error_overlap[(name1, name2)] = {
                    'both_wrong': both_wrong,
                    'only1': only1_wrong,
                    'only2': only2_wrong,
                    'overlap_ratio': overlap_ratio
                }
    
    # 7. åˆ†æäº’è¡¥æ€§ï¼ˆä¸€ä¸ªæ¨¡å‹é”™ä½†å¦ä¸€ä¸ªå¯¹çš„æƒ…å†µï¼‰
    logger.info("\nğŸ”„ æ¨¡å‹äº’è¡¥æ€§åˆ†æ:")
    best_model_idx = np.argmax([model_errors[name]['accuracy'] for name in model_names])
    best_model_name = model_names[best_model_idx]
    best_errors = model_errors[best_model_name]['error_mask']
    
    for i, name in enumerate(model_names):
        if i != best_model_idx:
            other_errors = model_errors[name]['error_mask']
            # æœ€ä½³æ¨¡å‹é”™ä½†å…¶ä»–æ¨¡å‹å¯¹çš„æ ·æœ¬
            best_wrong_other_right = np.sum(best_errors & ~other_errors)
            # æœ€ä½³æ¨¡å‹å¯¹ä½†å…¶ä»–æ¨¡å‹é”™çš„æ ·æœ¬
            best_right_other_wrong = np.sum(~best_errors & other_errors)
            # äº’è¡¥æ€§ï¼šå…¶ä»–æ¨¡å‹èƒ½çº æ­£æœ€ä½³æ¨¡å‹çš„é”™è¯¯
            complementarity = best_wrong_other_right / np.sum(best_errors) if np.sum(best_errors) > 0 else 0
            logger.info(f"   {name} å¯¹ {best_model_name} çš„äº’è¡¥æ€§:")
            logger.info(f"      {best_model_name}é”™ä½†{name}å¯¹: {best_wrong_other_right}")
            logger.info(f"      {best_model_name}å¯¹ä½†{name}é”™: {best_right_other_wrong}")
            logger.info(f"      äº’è¡¥æ€§æ¯”ç‡: {complementarity:.4f}")
    
    # 8. åˆ†æé›†æˆå¯èƒ½å¤±è´¥çš„åŸå› 
    logger.info("\nğŸ’¡ é›†æˆæ•ˆæœåˆ†æ:")
    logger.info(f"   æœ€ä½³å•æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {model_errors[best_model_name]['accuracy']:.4f})")
    
    # è®¡ç®—å¦‚æœä½¿ç”¨å¤šæ•°æŠ•ç¥¨çš„ç»“æœ
    from scipy import stats
    majority_vote = stats.mode(all_predictions, axis=0)[0].flatten()
    majority_acc = accuracy_score(y_val, majority_vote)
    logger.info(f"   å¤šæ•°æŠ•ç¥¨å‡†ç¡®ç‡: {majority_acc:.4f}")
    
    if majority_acc < model_errors[best_model_name]['accuracy']:
        logger.info(f"   âš ï¸  å¤šæ•°æŠ•ç¥¨ä¸å¦‚æœ€ä½³å•æ¨¡å‹ï¼Œä¸‹é™: {model_errors[best_model_name]['accuracy'] - majority_acc:.4f}")
        logger.info("\n   å¯èƒ½åŸå› :")
        
        # åŸå› 1: å¼±æ¨¡å‹æ‹–ç´¯
        weak_models = [name for name in model_names if model_errors[name]['accuracy'] < model_errors[best_model_name]['accuracy'] - 0.05]
        if weak_models:
            logger.info(f"   1. å¼±æ¨¡å‹æ‹–ç´¯: {', '.join(weak_models)} è¡¨ç°æ˜æ˜¾è¾ƒå·®")
        
        # åŸå› 2: é”™è¯¯é«˜åº¦é‡å 
        avg_overlap = np.mean([v['overlap_ratio'] for v in error_overlap.values()])
        if avg_overlap > 0.7:
            logger.info(f"   2. æ¨¡å‹é”™è¯¯é«˜åº¦é‡å  (å¹³å‡é‡å ç‡: {avg_overlap:.4f})ï¼Œç¼ºä¹å¤šæ ·æ€§")
        
        # åŸå› 3: æœ€ä½³æ¨¡å‹å·²ç»å¾ˆå¥½ï¼Œå…¶ä»–æ¨¡å‹æ— æ³•æä¾›æœ‰æ•ˆè¡¥å……
        if model_errors[best_model_name]['accuracy'] > 0.8:
            logger.info(f"   3. æœ€ä½³æ¨¡å‹å·²ç»è¡¨ç°å¾ˆå¥½ ({model_errors[best_model_name]['accuracy']:.4f})ï¼Œé›†æˆæ”¶ç›Šæœ‰é™")
        
        # åŸå› 4: åˆ†æ­§æ ·æœ¬ä¸­ï¼Œå¼±æ¨¡å‹ç»å¸¸å å¤šæ•°
        if n_disagree > 0:
            disagree_predictions = all_predictions[:, disagreements]
            disagree_labels = y_val[disagreements]
            disagree_majority = stats.mode(disagree_predictions, axis=0)[0].flatten()
            disagree_majority_acc = accuracy_score(disagree_labels, disagree_majority)
            logger.info(f"   4. åœ¨åˆ†æ­§æ ·æœ¬ä¸­ï¼Œå¤šæ•°æŠ•ç¥¨å‡†ç¡®ç‡: {disagree_majority_acc:.4f}")
            if disagree_majority_acc < 0.5:
                logger.info(f"      åœ¨åˆ†æ­§æ ·æœ¬ä¸­ï¼Œå¤šæ•°æŠ•ç¥¨è¡¨ç°å¾ˆå·®ï¼Œè¯´æ˜å¼±æ¨¡å‹åœ¨åˆ†æ­§æ—¶å ä¸»å¯¼")
    
    logger.info("\n" + "="*60)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_data = {
        'agreement_matrix': agreement_matrix.tolist(),
        'model_names': model_names,
        'n_agree': int(n_agree),
        'n_disagree': int(n_disagree),
        'model_errors': {name: {
            'accuracy': float(model_errors[name]['accuracy']),
            'n_errors': int(model_errors[name]['n_errors'])
        } for name in model_names},
        'error_overlap': {f"{k[0]}_vs_{k[1]}": {
            'both_wrong': int(v['both_wrong']),
            'only1': int(v['only1']),
            'only2': int(v['only2']),
            'overlap_ratio': float(v['overlap_ratio'])
        } for k, v in error_overlap.items()},
        'majority_vote_accuracy': float(majority_acc),
        'best_single_accuracy': float(model_errors[best_model_name]['accuracy'])
    }
    
    analysis_path = save_dir / 'model_analysis.json'
    import json
    with open(analysis_path, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, indent=2, ensure_ascii=False)
    logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜: {analysis_path}")


def generate_visualizations(ensemble, datasets, results, save_dir, logger):
    """ç”Ÿæˆé›†æˆæ¨¡å‹çš„å¯è§†åŒ–ç»“æœ
    
    å‚æ•°:
        ensemble: VotingEnsemble å¯¹è±¡
        datasets: æ•°æ®é›†å­—å…¸ï¼Œé”®ä¸º 'train'/'val'/'test'ï¼Œå€¼ä¸º (X, y) å…ƒç»„
        results: è¯„ä¼°ç»“æœå­—å…¸
        save_dir: ä¿å­˜ç›®å½•
        logger: Logger å¯¹è±¡
    """
    figures_dir = Path(save_dir) / 'figures'
    figures_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. PR æ›²çº¿
    try:
        has_pr_data = any(
            isinstance(results.get(split), dict) and (
                results[split].get('precision_curve') is not None or
                isinstance(results[split].get('pr_curve'), dict)
            )
            for split in ['train', 'val', 'test']
        )
        if has_pr_data:
            method_name = "Stacking" if isinstance(ensemble, StackingEnsemble) else "æŠ•ç¥¨ï¼ˆ{}ï¼‰".format("ç¡¬" if getattr(ensemble, 'voting', 'soft') == 'hard' else "è½¯")
            plot_split_metrics(
                results,
                metric='pr',
                title="é›†æˆæ¨¡å‹ PR æ›²çº¿ï¼ˆ{}ï¼‰".format(method_name),
                save_path=figures_dir / "ensemble_pr_curve.png"
            )
            logger.info("ğŸ“Š PR æ›²çº¿å·²ä¿å­˜")
    except Exception as exc:
        logger.debug(f"ç”Ÿæˆ PR æ›²çº¿å¤±è´¥: {exc}")
    
    # 2. æ··æ·†çŸ©é˜µ
    for split in ['train', 'val', 'test']:
        X, y = datasets.get(split, (None, None))
        if X is None or y is None:
            continue
        
        try:
            y_pred = ensemble.predict(X)
            class_names = ['Cat', 'Dog']
            from sklearn import metrics
            cm = metrics.confusion_matrix(y, y_pred)
            method_name = "Stacking" if isinstance(ensemble, StackingEnsemble) else "æŠ•ç¥¨ï¼ˆ{}ï¼‰".format("ç¡¬" if getattr(ensemble, 'voting', 'soft') == 'hard' else "è½¯")
            plot_confusion_matrix(
                cm,
                class_names=class_names,
                title="é›†æˆæ¨¡å‹ {} æ··æ·†çŸ©é˜µï¼ˆ{}ï¼‰".format(
                    split.capitalize(),
                    method_name
                ),
                save_path=figures_dir / "ensemble_{}_confusion.png".format(split)
            )
            logger.info(f"ğŸ“Š {split} æ··æ·†çŸ©é˜µå·²ä¿å­˜")
        except Exception as exc:
            logger.debug(f"ç”Ÿæˆ {split} æ··æ·†çŸ©é˜µå¤±è´¥: {exc}")
        
        # 3. ROC æ›²çº¿
        try:
            y_scores = ensemble.predict_proba(X)[:, 1]
            if len(np.unique(y)) == 2:
                plot_roc_curve(
                    y,
                    y_scores,
                    title="é›†æˆæ¨¡å‹ {} ROC æ›²çº¿".format(split.capitalize()),
                    save_path=figures_dir / "ensemble_{}_roc.png".format(split)
                )
                logger.info(f"ğŸ“Š {split} ROC æ›²çº¿å·²ä¿å­˜")
        except Exception as exc:
            logger.debug(f"ç”Ÿæˆ {split} ROC æ›²çº¿å¤±è´¥: {exc}")


def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œé›†æˆå­¦ä¹ æŠ•ç¥¨é¢„æµ‹"""
    parser = build_parser()
    args = parser.parse_args()
    
    # ç”Ÿæˆæ—¶é—´æˆ³å’Œä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    if args.save_dir is None:
        args.save_dir = f"runs/ensemble_{timestamp}"
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = Logger(
        name="ensemble_voting",
        log_dir=str(save_dir),
        filename="ensemble_voting.log",
        level=args.log_level,
    )
    
    start_time = time.time()
    
    try:
        # æ—¥å¿—é…ç½®ä¿¡æ¯
        cfg_lines = [
            "é›†æˆæ–¹æ³•: {}".format(args.method),
            "æ•°æ®ç›®å½•: {}".format(args.data_dir),
            "ä¿å­˜ç›®å½•: {}".format(save_dir),
        ]
        if args.method == 'voting':
            cfg_lines.append("æŠ•ç¥¨æ–¹å¼: {}".format(args.voting))
        elif args.method == 'stacking':
            cfg_lines.append("äº¤å‰éªŒè¯æŠ˜æ•°: {}".format(args.stacking_cv))
            cfg_lines.append("ä½¿ç”¨æ¦‚ç‡ç‰¹å¾: {}".format(args.stacking_use_proba))
        if args.auto_weights:
            cfg_lines.append("æƒé‡ç­–ç•¥: è‡ªåŠ¨è®¡ç®—ï¼ˆåŸºäºéªŒè¯é›†æ€§èƒ½ï¼Œå¹‚æ¬¡={}ï¼‰".format(args.weight_power))
        elif args.weights:
            cfg_lines.append("æ¨¡å‹æƒé‡: {}".format(args.weights))
        else:
            cfg_lines.append("æƒé‡ç­–ç•¥: ç­‰æƒé‡")
        if args.top_k is not None:
            cfg_lines.append("æ¨¡å‹ç­›é€‰: åªä½¿ç”¨è¡¨ç°æœ€å¥½çš„ {} ä¸ªæ¨¡å‹".format(args.top_k))
        if args.min_accuracy is not None:
            cfg_lines.append("å‡†ç¡®ç‡é˜ˆå€¼: æ’é™¤å‡†ç¡®ç‡ < {} çš„æ¨¡å‹".format(args.min_accuracy))
        logger.block("å¼€å§‹é›†æˆå­¦ä¹ æŠ•ç¥¨é¢„æµ‹", cfg_lines)
        
        # 1. åŠ è½½æ¨¡å‹
        logger.info("ğŸ“¥ åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹...")
        lr_model = load_model(args.lr_model, "é€»è¾‘å›å½’")
        rf_model = load_model(args.rf_model, "éšæœºæ£®æ—")
        svm_model = load_model(args.svm_model, "SVM")
        
        models = [lr_model, rf_model, svm_model]
        model_names = ["é€»è¾‘å›å½’", "éšæœºæ£®æ—", "SVM"]
        
        # 2. åŠ è½½æ•°æ®
        logger.info("ğŸ“Š åŠ è½½æ•°æ®é›†...")
        datasets = load_train_val_test(
            args.data_dir,
            args.train_dirname,
            args.val_dirname,
            args.test_dirname,
            logger=logger
        )
        
        # 3. è¯„ä¼°å•ä¸ªæ¨¡å‹æ€§èƒ½ï¼ˆç”¨äºè‡ªåŠ¨æƒé‡è®¡ç®—å’Œæ¨¡å‹ç­›é€‰ï¼‰
        weights = args.weights
        model_scores = None
        
        # å¦‚æœéœ€è¦è‡ªåŠ¨æƒé‡æˆ–æ¨¡å‹ç­›é€‰ï¼Œå…ˆè¯„ä¼°æ‰€æœ‰æ¨¡å‹
        if args.auto_weights or args.top_k is not None or args.min_accuracy is not None:
            logger.info("ğŸ“Š è¯„ä¼°å•ä¸ªæ¨¡å‹æ€§èƒ½...")
            X_val, y_val = datasets.get('val', (None, None))
            if X_val is None or y_val is None:
                logger.warning("âš ï¸  éªŒè¯é›†ä¸å­˜åœ¨ï¼Œæ— æ³•è¯„ä¼°æ¨¡å‹æ€§èƒ½")
                if args.auto_weights:
                    logger.warning("   æ— æ³•è‡ªåŠ¨è®¡ç®—æƒé‡ï¼Œä½¿ç”¨ç­‰æƒé‡")
                    weights = None
            else:
                model_scores = []
                for model, name in zip(models, model_names):
                    try:
                        y_pred = model.predict(X_val)
                        from sklearn.metrics import accuracy_score
                        acc = accuracy_score(y_val, y_pred)
                        model_scores.append(acc)
                        logger.info(f"   {name}: éªŒè¯é›†å‡†ç¡®ç‡ = {acc:.4f}")
                    except Exception as e:
                        logger.warning(f"   {name}: è¯„ä¼°å¤±è´¥ ({e})ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°")
                        model_scores.append(0.0)  # é»˜è®¤åˆ†æ•°
                
                model_scores = np.array(model_scores)
                
                # æ ¹æ®å‡†ç¡®ç‡ç­›é€‰æ¨¡å‹
                if args.min_accuracy is not None:
                    keep_mask = model_scores >= args.min_accuracy
                    if not np.any(keep_mask):
                        logger.warning("âš ï¸  æ‰€æœ‰æ¨¡å‹éƒ½è¢«è¿‡æ»¤ï¼Œä½¿ç”¨æ‰€æœ‰æ¨¡å‹")
                        keep_mask = np.ones(len(models), dtype=bool)
                    else:
                        filtered_count = np.sum(~keep_mask)
                        if filtered_count > 0:
                            logger.info(f"ğŸ” è¿‡æ»¤æ‰ {filtered_count} ä¸ªä½æ€§èƒ½æ¨¡å‹ï¼ˆå‡†ç¡®ç‡ < {args.min_accuracy:.4f}ï¼‰")
                            models = [m for m, keep in zip(models, keep_mask) if keep]
                            model_names = [n for n, keep in zip(model_names, keep_mask) if keep]
                            model_scores = model_scores[keep_mask]
                
                # æ ¹æ® top_k ç­›é€‰æ¨¡å‹
                if args.top_k is not None and args.top_k < len(models):
                    top_indices = np.argsort(model_scores)[-args.top_k:][::-1]
                    logger.info(f"ğŸ” åªä½¿ç”¨è¡¨ç°æœ€å¥½çš„ {args.top_k} ä¸ªæ¨¡å‹")
                    models = [models[i] for i in top_indices]
                    model_names = [model_names[i] for i in top_indices]
                    model_scores = model_scores[top_indices]
                    logger.info(f"   é€‰ä¸­çš„æ¨¡å‹: {', '.join(model_names)}")
                
                # è‡ªåŠ¨è®¡ç®—æƒé‡
                if args.auto_weights and model_scores is not None:
                    if np.all(model_scores > 0):
                        # ä½¿ç”¨æŒ‡å®šçš„å¹‚æ¬¡æ¥å¢å¼ºå¥½æ¨¡å‹çš„æƒé‡
                        weights = (model_scores ** args.weight_power).tolist()
                        logger.info(f"âœ… è‡ªåŠ¨è®¡ç®—æƒé‡ï¼ˆå¹‚æ¬¡={args.weight_power}ï¼‰: {weights}")
                    else:
                        weights = model_scores.tolist()
                        logger.info(f"âœ… è‡ªåŠ¨è®¡ç®—æƒé‡: {weights}")
        
        # æ£€æŸ¥æ¨¡å‹æ•°é‡
        if len(models) == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œé›†æˆ")
        if len(models) == 1:
            logger.warning("âš ï¸  åªæœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œé›†æˆæ•ˆæœç­‰åŒäºå•ä¸ªæ¨¡å‹")
        
        # 4. åˆ›å»ºé›†æˆæ¨¡å‹
        if args.method == 'voting':
            logger.info("ğŸ”§ åˆ›å»ºæŠ•ç¥¨é›†æˆæ¨¡å‹...")
            ensemble = VotingEnsemble(
                models=models,
                model_names=model_names,
                voting=args.voting,
                weights=weights,
            )
            ensemble.logger = logger
            
            voting_type = "ç¡¬æŠ•ç¥¨" if args.voting == 'hard' else "è½¯æŠ•ç¥¨"
            logger.info(f"âœ… é›†æˆæ¨¡å‹åˆ›å»ºå®Œæˆï¼ˆ{voting_type}ï¼‰")
            if weights:
                weight_info = ", ".join(
                    f"{name}={w:.3f}" for name, w in zip(model_names, ensemble.weights)
                )
                logger.info(f"   æƒé‡: {weight_info}")
            
            # 5. è¯„ä¼°æ¨¡å‹
            logger.info("ğŸ“ˆ è¯„ä¼°é›†æˆæ¨¡å‹...")
            results = {}
            for split_name in ['train', 'val', 'test']:
                X, y = datasets.get(split_name, (None, None))
                if X is not None and y is not None:
                    results[split_name] = ensemble.evaluate(X, y, name=split_name.capitalize())
                else:
                    results[split_name] = None
                    
        elif args.method == 'stacking':
            logger.info("ğŸ”§ åˆ›å»º Stacking é›†æˆæ¨¡å‹...")
            ensemble = StackingEnsemble(
                base_models=models,
                base_model_names=model_names,
                cv_folds=args.stacking_cv,
                use_proba=args.stacking_use_proba,
                random_state=42,
            )
            ensemble.logger = logger
            
            # è®­ç»ƒ Stacking æ¨¡å‹
            X_train, y_train = datasets.get('train', (None, None))
            if X_train is None or y_train is None:
                raise ValueError("è®­ç»ƒé›†ä¸å­˜åœ¨ï¼Œæ— æ³•è®­ç»ƒ Stacking æ¨¡å‹")
            
            ensemble.fit(X_train, y_train)
            logger.info("âœ… Stacking é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            # 5. è¯„ä¼°æ¨¡å‹
            logger.info("ğŸ“ˆ è¯„ä¼°é›†æˆæ¨¡å‹...")
            results = {}
            for split_name in ['train', 'val', 'test']:
                X, y = datasets.get(split_name, (None, None))
                if X is not None and y is not None:
                    results[split_name] = ensemble.evaluate(X, y, name=split_name.capitalize())
                else:
                    results[split_name] = None
        else:
            raise ValueError(f"æœªçŸ¥çš„é›†æˆæ–¹æ³•: {args.method}")
        
        # 6. æ¨¡å‹è¯Šæ–­åˆ†æ
        if args.analyze:
            logger.info("ğŸ” è¿›è¡Œæ¨¡å‹è¯Šæ–­åˆ†æ...")
            analyze_models(models, model_names, datasets, save_dir, logger)
        
        # 7. ç”Ÿæˆå¯è§†åŒ–
        logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        generate_visualizations(ensemble, datasets, results, save_dir, logger)
        
        # 8. ä¿å­˜ç»“æœ
        import json
        weights_list = None
        if args.method == 'voting' and (args.weights or args.auto_weights):
            if isinstance(ensemble, VotingEnsemble):
                if isinstance(ensemble.weights, np.ndarray):
                    weights_list = ensemble.weights.tolist()
                else:
                    weights_list = list(ensemble.weights)
        results_data = {
            'timestamp': datetime.now().isoformat(),
            'method': args.method,
            'voting': args.voting if args.method == 'voting' else None,
            'weights': weights_list,
            'auto_weights': args.auto_weights,
            'stacking_cv': args.stacking_cv if args.method == 'stacking' else None,
            'stacking_use_proba': args.stacking_use_proba if args.method == 'stacking' else None,
            'model_paths': {
                'lr': str(args.lr_model),
                'rf': str(args.rf_model),
                'svm': str(args.svm_model),
            },
            'results': results,
        }
        results_path = save_dir / 'ensemble_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_path}")
        
        # 9. æ€»ç»“å’Œæ€§èƒ½å¯¹æ¯”
        total_time = time.time() - start_time
        best_acc = None
        if results.get("val") and results["val"].get("accuracy") is not None:
            best_acc = results["val"]["accuracy"]
        elif results.get("test") and results["test"].get("accuracy") is not None:
            best_acc = results["test"]["accuracy"]
        
        summary_lines = [
            "è€—æ—¶: {}".format(Logger.format_duration(total_time)),
            "éªŒè¯é›†å‡†ç¡®ç‡: {:.4f}".format(results["val"]["accuracy"]) if results.get("val") and results["val"].get("accuracy") is not None else "éªŒè¯é›†å‡†ç¡®ç‡: -",
            "æµ‹è¯•é›†å‡†ç¡®ç‡: {:.4f}".format(results["test"]["accuracy"]) if results.get("test") and results["test"].get("accuracy") is not None else "æµ‹è¯•é›†å‡†ç¡®ç‡: -",
            "ç»“æœç›®å½•: {}".format(save_dir),
        ]
        
        # å¦‚æœè¯„ä¼°äº†å•ä¸ªæ¨¡å‹ï¼Œæ˜¾ç¤ºå¯¹æ¯”
        if model_scores is not None and len(model_scores) > 0:
            best_single_acc = np.max(model_scores)
            ensemble_val_acc = results.get("val", {}).get("accuracy")
            if ensemble_val_acc is not None:
                improvement = ensemble_val_acc - best_single_acc
                if improvement > 0.001:  # æå‡è¶…è¿‡0.1%
                    summary_lines.append("ğŸ“ˆ ç›¸æ¯”æœ€ä½³å•æ¨¡å‹ï¼ˆéªŒè¯é›†ï¼‰æå‡: +{:.4f}".format(improvement))
                elif improvement < -0.001:  # ä¸‹é™è¶…è¿‡0.1%
                    summary_lines.append("ğŸ“‰ ç›¸æ¯”æœ€ä½³å•æ¨¡å‹ï¼ˆéªŒè¯é›†ï¼‰ä¸‹é™: {:.4f}".format(improvement))
                else:
                    summary_lines.append("â¡ï¸  ä¸æœ€ä½³å•æ¨¡å‹ï¼ˆéªŒè¯é›†ï¼‰æŒå¹³")
        
        logger.block("é›†æˆå­¦ä¹ å®Œæˆ", summary_lines)
        
        return ensemble, results
        
    except Exception as e:
        logger.exception("é›†æˆå­¦ä¹ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: %s", e)
        raise


if __name__ == "__main__":
    main()

