"""
ç»Ÿä¸€çš„æ¨¡å‹æ¨ç†è„šæœ¬ã€‚

è¯¥è„šæœ¬æ”¯æŒåŠ è½½ PyTorch è®­ç»ƒäº§ç”Ÿçš„æ£€æŸ¥ç‚¹ï¼Œå¯¹å•å¼ å›¾ç‰‡æˆ–ç›®å½•æ‰¹é‡æ‰§è¡ŒçŒ«ç‹—åˆ†ç±»æ¨ç†ï¼Œ
å¹¶å°†é¢„æµ‹ç»“æœå¯¼å‡ºä¸º CSV / JSON æ–‡ä»¶ã€‚
"""

import argparse
from datetime import datetime
from pathlib import Path

import torch

from src.utils.inference_utils import (
    collect_image_paths,
    prepare_model,
    run_inference,
    summarize_predictions,
    CLASS_NAMES,
)
from src.data.data_utils import build_transforms
from src.utils.logger import Logger
from tools.reporting import save_predictions_to_csv, save_predictions_to_json


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    
    è¿”å›:
        argparse.Namespace: å‚æ•°å¯¹è±¡ã€‚
    """
    description = "çŒ«ç‹—åˆ†ç±»æ¨¡å‹æ¨ç†è„šæœ¬"
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument("--input", required=True, help="è¾“å…¥èµ„æºè·¯å¾„ï¼Œå¯ä¸ºå›¾ç‰‡/ç›®å½•/TXT/CSV åˆ—è¡¨ã€‚")
    parser.add_argument("--checkpoint", required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œä¾‹å¦‚ runs/torch_cnn/best.ptã€‚")
    parser.add_argument("--arch", default=None, help="æ¨¡å‹æ¶æ„æ ‡è¯†ï¼ˆå¯é€‰ï¼Œè¦†ç›–æ£€æŸ¥ç‚¹é…ç½®ï¼‰ã€‚")
    parser.add_argument("--device", default="auto", help="æ¨ç†è®¾å¤‡ï¼Œä¾‹å¦‚ cpu / cuda:0ï¼Œé»˜è®¤è‡ªåŠ¨é€‰æ‹©ã€‚")
    parser.add_argument("--image-size", type=int, default=None, help="æ¨ç†å›¾åƒå°ºå¯¸ï¼Œé»˜è®¤è¯»å–æ£€æŸ¥ç‚¹é…ç½®ã€‚")
    parser.add_argument("--normalize-imagenet", dest="normalize_imagenet", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨ ImageNet å½’ä¸€åŒ–ã€‚")
    parser.add_argument("--batch-size", type=int, default=32, help="æ¨ç†æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ 32ã€‚")
    parser.add_argument("--threshold", type=float, default=0.5, help="åˆ¤æ–­ä¸ºç‹—çš„æ¦‚ç‡é˜ˆå€¼ï¼Œé»˜è®¤ 0.5ã€‚")
    parser.add_argument("--output-csv", dest="output_csv", default=None, help="é¢„æµ‹ç»“æœ CSV è¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤å†™å…¥ runs/inference/ ç›®å½•ã€‚")
    parser.add_argument("--output-json", dest="output_json", default=None, help="é¢å¤–å¯¼å‡º JSON ç»“æœçš„è·¯å¾„ï¼ˆå¯é€‰ï¼‰ã€‚")
    parser.add_argument("--no-recursive", action="store_true", help="å¤„ç†ç›®å½•æ—¶ä¸é€’å½’æœç´¢å­ç›®å½•ã€‚")
    parser.add_argument("--quiet", action="store_true", help="å…³é—­è¿›åº¦æ¡ä¸è¯¦ç»†æ—¥å¿—ã€‚")
    return parser.parse_args()


def _resolve_device(device_arg):
    """æ ¹æ®å‚æ•°è§£ææ¨ç†è®¾å¤‡æ ‡è¯†ã€‚"""
    if device_arg and device_arg.lower() != "auto":
        return device_arg
    return "cuda" if torch.cuda.is_available() else "cpu"


def _prepare_csv_path(csv_path):
    """ç”Ÿæˆ CSV è¾“å‡ºè·¯å¾„ã€‚"""
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    base_dir = Path("runs") / "inference"
    base_dir.mkdir(parents=True, exist_ok=True)
    if csv_path is None:
        csv_path = base_dir / "predictions_{}.csv".format(timestamp)
    return str(Path(csv_path).expanduser())


def _log_summary(logger, summary):
    """è®°å½•æ¨ç†æ‘˜è¦ä¿¡æ¯ã€‚"""
    total = summary["total"]
    cats = summary["cats"]
    dogs = summary["dogs"]
    logger.block(
        "ğŸ“Š æ¨ç†ç»Ÿè®¡",
        [
            "æ€»ä»»åŠ¡æ•°: {}".format(total),
            "çŒ«é¢„æµ‹: {} å¼  | å¹³å‡ç½®ä¿¡åº¦ {:.2%}".format(cats["count"], cats["avg_prob"]),
            "ç‹—é¢„æµ‹: {} å¼  | å¹³å‡ç½®ä¿¡åº¦ {:.2%}".format(dogs["count"], dogs["avg_prob"]),
        ],
    )


def main():
    """è„šæœ¬ä¸»å…¥å£ã€‚"""
    args = parse_args()
    logger = Logger(name="infer")
    device = _resolve_device(args.device)
    logger.block(
        "ğŸš€ æ¨ç†å¯åŠ¨",
        [
            "è®¾å¤‡: {}".format(device.upper()),
            "æ£€æŸ¥ç‚¹: {}".format(args.checkpoint),
        ],
    )
    model, config = prepare_model(args.checkpoint, device=device, arch=args.arch)
    if args.image_size is not None:
        image_size = args.image_size
    else:
        image_size = config.get("image_size", 224)
    normalize_imagenet = config.get("normalize_imagenet", False)
    if args.normalize_imagenet is not None:
        normalize_imagenet = args.normalize_imagenet
    transform = build_transforms(
        size=image_size,
        augment=False,
        use_imagenet_norm=normalize_imagenet,
    )
    logger.block(
        "ğŸ§® æ¨ç†é…ç½®",
        [
            "å›¾åƒå°ºå¯¸: {}x{}".format(image_size, image_size),
            "æ ‡å‡†åŒ–: {}".format("ImageNet" if normalize_imagenet else "é»˜è®¤ [0,1]"),
            "æ‰¹æ¬¡å¤§å°: {}".format(args.batch_size),
            "æ¦‚ç‡é˜ˆå€¼: {:.2f}".format(args.threshold),
        ],
    )
    recursive = not args.no_recursive
    logger.info("å¼€å§‹æ”¶é›†å›¾åƒèµ„æº: %s", args.input)
    image_paths = collect_image_paths(args.input, recursive=recursive)
    logger.info("æ”¶é›†å®Œæˆï¼Œå…± %d å¼ å›¾åƒã€‚", len(image_paths))
    results = run_inference(
        model,
        image_paths,
        transform,
        device,
        batch_size=args.batch_size,
        threshold=args.threshold,
        show_progress=not args.quiet,
        logger=logger.raw,
    )
    summary = summarize_predictions(results)
    _log_summary(logger, summary)
    csv_path = _prepare_csv_path(args.output_csv)
    saved_csv = save_predictions_to_csv(results, csv_path)
    logger.info("ğŸ’¾ å·²ä¿å­˜ CSV ç»“æœ: %s", saved_csv)
    if args.output_json is not None:
        saved_json = save_predictions_to_json(results, summary, args.output_json, class_names=CLASS_NAMES)
        logger.info("ğŸ’¾ å·²ä¿å­˜ JSON ç»“æœ: %s", saved_json)
    else:
        saved_json = None
    if saved_json is None and args.output_json is None:
        logger.info("â„¹ï¸ å¦‚éœ€ JSON ç»“æœï¼Œå¯ä½¿ç”¨ --output-json æŒ‡å®šè¾“å‡ºè·¯å¾„ã€‚")
    logger.info("âœ¨ æ¨ç†æµç¨‹å®Œæˆã€‚")


if __name__ == "__main__":
    main()
