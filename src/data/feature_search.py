import argparse
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.decomposition import PCA


current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.data.feature_extraction import (  # noqa: E402
    FEATURE_POOL,
    UnifiedFeatureExtractor,
    collect_image_paths_and_labels,
    save_features_to_file,
)


LOGGER = logging.getLogger('feature_search')


def align_and_concatenate_features(feature_payloads):
    """å¯¹é½å¤šä¸ªç‰¹å¾çŸ©é˜µå¹¶æ°´å¹³æ‹¼æ¥ã€‚
    
    å‚æ•°:
        feature_payloads: listï¼Œå•ç‰¹å¾æå–ç»“æœå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« 'X', 'y', 'valid_indices'
        
    è¿”å›:
        tuple: (X_concat, y_aligned, common_indices)
            - X_concat: numpy.ndarrayï¼Œå¯¹é½å¹¶æ‹¼æ¥åçš„ç‰¹å¾çŸ©é˜µ
            - y_aligned: numpy.ndarray æˆ– Noneï¼Œå¯¹é½åçš„æ ‡ç­¾
            - common_indices: listï¼Œå…±åŒæ ·æœ¬çš„åŸå§‹ç´¢å¼•
            
    å¼‚å¸¸:
        RuntimeError: å½“ç‰¹å¾æ²¡æœ‰å…±åŒæ ·æœ¬æ—¶æŠ›å‡º
        
    ç¤ºä¾‹:
        >>> payload1 = {'X': X1, 'y': y1, 'valid_indices': [0, 1, 2]}
        >>> payload2 = {'X': X2, 'y': y2, 'valid_indices': [1, 2, 3]}
        >>> X, y, indices = align_and_concatenate_features([payload1, payload2])
        >>> # X å½¢çŠ¶: (2, dim1 + dim2)ï¼Œindices: [1, 2]
    """
    if len(feature_payloads) == 1:
        single = feature_payloads[0]
        return single['X'], single.get('y'), list(single['valid_indices'])
    
    # æ‰¾åˆ°æ‰€æœ‰ç‰¹å¾çš„å…±åŒæ ·æœ¬ç´¢å¼•
    index_sets = [set(payload['valid_indices']) for payload in feature_payloads]
    common_indices = sorted(set.intersection(*index_sets))
    if not common_indices:
        raise RuntimeError('å„ç‰¹å¾æ²¡æœ‰å…±åŒæ ·æœ¬ï¼Œæ— æ³•å¯¹é½')
    
    # æ„å»ºç´¢å¼•æ˜ å°„å¹¶å¯¹é½ç‰¹å¾çŸ©é˜µ
    aligned_blocks = []
    for payload in feature_payloads:
        position_map = {idx: pos for pos, idx in enumerate(payload['valid_indices'])}
        rows = [position_map[idx] for idx in common_indices]
        aligned_blocks.append(payload['X'][rows])
    
    X_concat = np.hstack(aligned_blocks).astype(np.float32)
    
    # å¯¹é½æ ‡ç­¾ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ª payload çš„æ ‡ç­¾ï¼‰
    ref_payload = feature_payloads[0]
    ref_position_map = {idx: pos for pos, idx in enumerate(ref_payload['valid_indices'])}
    ref_rows = [ref_position_map[idx] for idx in common_indices]
    y_aligned = None
    if ref_payload.get('y') is not None:
        y_aligned = ref_payload['y'][ref_rows]
    
    return X_concat, y_aligned, common_indices


class FeatureSearch:
    """ç‰¹å¾ç»„åˆæœç´¢å™¨ï¼Œä½¿ç”¨æŸæœç´¢æ–¹æ³•å¯»æ‰¾æœ€ä¼˜ç‰¹å¾ç»„åˆã€‚
    
    å°è£…ç‰¹å¾æå–ã€è¯„ä¼°å’Œæœç´¢æµç¨‹ï¼Œæ”¯æŒé…ç½®ç®¡ç†å’ŒçŠ¶æ€ç»´æŠ¤ã€‚
    """
    
    def __init__(self, model='svm', cv=5, scoring='accuracy', beam_width=5, 
                 enable_cache=True, image_size=128, n_jobs=None, pca_components=512):
        """åˆå§‹åŒ–ç‰¹å¾æœç´¢å™¨ã€‚
        
        å‚æ•°:
            model: strï¼Œè¯„ä¼°æ¨¡å‹ (svm/rf)
            cv: intï¼Œäº¤å‰éªŒè¯æŠ˜æ•°
            scoring: strï¼Œè¯„ä¼°æŒ‡æ ‡
            beam_width: intï¼ŒæŸæœç´¢çš„æŸå®½ï¼ˆå»ºè®® 3-10ï¼Œé»˜è®¤ 5ï¼‰
            enable_cache: boolï¼Œæ˜¯å¦å¯ç”¨è¯„ä¼°ç»“æœç¼“å­˜
            image_size: intï¼Œå›¾åƒç¼©æ”¾å°ºå¯¸
            n_jobs: int æˆ– Noneï¼Œç‰¹å¾æå–å¹¶è¡Œçº¿ç¨‹æ•°
            pca_components: int æˆ– Noneï¼ŒPCAé™ç»´åçš„ç»´åº¦ï¼ˆNoneè¡¨ç¤ºä¸é™ç»´ï¼Œé»˜è®¤512ï¼‰
        """
        self.model_name = model
        self.cv = cv
        self.scoring = scoring
        self.beam_width = beam_width
        self.enable_cache = enable_cache
        self.image_size = image_size
        self.n_jobs = n_jobs
        self.pca_components = pca_components
        
        # çŠ¶æ€å˜é‡
        self.feature_cache = {}
        self.evaluation_cache = {} if enable_cache else None
        self.search_history = []
        self.best_result = None
        self.dataset_dir = None
        self.split = None
        self.sample_ratio = None
        self.image_paths = None
        self.labels = None
    
    def _evaluate_subset_cv(self, X, y, subset_names):
        """ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°ç‰¹å¾å­é›†ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰ã€‚
        
        å‚æ•°:
            X: numpy.ndarrayï¼Œç‰¹å¾çŸ©é˜µ
            y: numpy.ndarrayï¼Œæ ‡ç­¾
            subset_names: listï¼Œç‰¹å¾å­é›†åç§°ï¼ˆç”¨äºç¼“å­˜é”®ï¼‰
            
        è¿”å›:
            dict: åŒ…å«å¾—åˆ†ã€æ–¹å·®ä¸é™ç»´ä¿¡æ¯çš„å­—å…¸
        """
        # ä½¿ç”¨ç‰¹å¾åç§°å’Œå½¢çŠ¶ç”Ÿæˆç¼“å­˜é”®
        if self.evaluation_cache is not None and subset_names is not None:
            cache_key = (tuple(sorted(subset_names)), X.shape, self.model_name, self.cv, self.scoring)
            if cache_key in self.evaluation_cache:
                self.evaluation_cache['_hits'] = self.evaluation_cache.get('_hits', 0) + 1
                return self.evaluation_cache[cache_key]
        else:
            cache_key = None
        
        n_features = X.shape[1] if len(X.shape) == 2 else 0
        
        # æ„å»º Pipeline
        steps = [('scaler', StandardScaler())]
        pca_components = 0
        if self.pca_components is not None and n_features > self.pca_components:
            pca_components = self.pca_components
            steps.append(('pca', PCA(n_components=pca_components, random_state=42)))  # type: ignore
        
        if self.model_name == 'svm':
            model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=False, random_state=42)
        elif self.model_name == 'rf':
            model = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)
        else:
            raise ValueError('æœªçŸ¥æ¨¡å‹: {}'.format(self.model_name))
        
        steps.append(('model', model))  # type: ignore
        pipeline = Pipeline(steps)
        
        # äº¤å‰éªŒè¯è¯„ä¼°
        splitter = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=42)
        start_time = time.time()
        scores = cross_val_score(pipeline, X, y, cv=splitter, scoring=self.scoring, n_jobs=-1)
        elapsed = time.time() - start_time
        
        result = {
            'score': float(np.mean(scores)),
            'std': float(np.std(scores)),
            'pca': int(pca_components),
            'original_dim': int(n_features),
            'time': elapsed,
        }
        
        if self.evaluation_cache is not None and cache_key is not None:
            self.evaluation_cache[cache_key] = result
        
        return result
    
    def _evaluate_subset(self, subset_names):
        """è¯„ä¼°ç‰¹å¾å­é›†å¹¶è¿”å›ç»“æœï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰ã€‚
        
        å‚æ•°:
            subset_names: listï¼Œç‰¹å¾å­é›†åç§°åˆ—è¡¨
            
        è¿”å›:
            dict: è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å« score, std, pca, original_dim
        """
        payloads = [self.feature_cache[name] for name in subset_names]
        X_sub, y_sub, _ = align_and_concatenate_features(payloads)
        result = self._evaluate_subset_cv(X_sub, y_sub, subset_names)
        return {
            'subset': subset_names,
            'score': result['score'],
            'std': result['std'],
            'pca': result['pca'],
            'original_dim': result['original_dim'],
        }
    
    @staticmethod
    def _sample_data(image_paths, labels, sample_ratio, seed=42):
        """å¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ï¼ˆé™æ€æ–¹æ³•ï¼‰ã€‚
        
        å‚æ•°:
            image_paths: listï¼Œå›¾åƒè·¯å¾„åˆ—è¡¨
            labels: listï¼Œæ ‡ç­¾åˆ—è¡¨
            sample_ratio: floatï¼Œé‡‡æ ·æ¯”ä¾‹ (0-1]
            seed: intï¼Œéšæœºç§å­
            
        è¿”å›:
            tuple: (é‡‡æ ·åçš„ image_paths, labels)
        """
        if sample_ratio >= 1.0:
            return image_paths, labels
        
        total = len(image_paths)
        size = max(1, int(total * sample_ratio))
        rng = np.random.default_rng(seed=seed)
        indices = rng.choice(total, size=size, replace=False)
        sampled_paths = [image_paths[i] for i in indices]
        sampled_labels = [labels[i] for i in indices]
        LOGGER.info('ğŸ“¦ é‡‡æ · %.0f%% æ•°æ®: %d -> %d', sample_ratio * 100, total, len(sampled_paths))
        return sampled_paths, sampled_labels
    
    def load_data(self, dataset_dir='dataset', split='train', sample_ratio=1.0):
        """åŠ è½½æ•°æ®é›†ã€‚
        
        å‚æ•°:
            dataset_dir: strï¼Œæ•°æ®é›†æ ¹ç›®å½•
            split: strï¼Œæ•°æ®é›†åˆ†å‰² (train/val/test)
            sample_ratio: floatï¼Œé‡‡æ ·æ¯”ä¾‹ (0-1]
            
        è¿”å›:
            tuple: (image_paths, labels)
        """
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
        self.dataset_dir = dataset_dir
        self.split = split
        self.sample_ratio = sample_ratio
        
        LOGGER.info('ğŸš€ å¯åŠ¨ç‰¹å¾æœç´¢')
        image_paths, labels = collect_image_paths_and_labels(dataset_dir, split)
        if not image_paths:
            raise RuntimeError('è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•è¿è¡Œæœç´¢')
        
        # ä¿å­˜å®Œæ•´æ•°æ®é›†è·¯å¾„ï¼ˆç”¨äºåç»­å®Œæ•´æ•°æ®é›†æå–ï¼‰
        self.image_paths = image_paths
        self.labels = labels
        
        image_paths, labels = self._sample_data(image_paths, labels, sample_ratio)
        if sample_ratio >= 1.0:
            LOGGER.info('ğŸ“¦ ä½¿ç”¨å…¨éƒ¨ %d å¼ å›¾åƒ', len(image_paths))
        
        return image_paths, labels
    
    def extract_features(self, image_paths, labels, feature_names=None):
        """æå–ç‰¹å¾å¹¶ç¼“å­˜ã€‚
        
        å‚æ•°:
            image_paths: listï¼Œå›¾åƒè·¯å¾„åˆ—è¡¨
            labels: listï¼Œæ ‡ç­¾åˆ—è¡¨
            feature_names: list æˆ– Noneï¼Œè¦æå–çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆNone è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
            
        è¿”å›:
            dict: ç‰¹å¾ç¼“å­˜å­—å…¸
        """
        if feature_names is None:
            feature_names = FEATURE_POOL
        
        self.feature_cache = {}
        for feature_name in feature_names:
            if feature_name not in FEATURE_POOL:
                LOGGER.warning('   è·³è¿‡æœªçŸ¥ç‰¹å¾: %s', feature_name)
                continue
                
            LOGGER.info('ğŸ”§ æå–ç‰¹å¾: %s', feature_name)
            try:
                payload = UnifiedFeatureExtractor.extract_single_feature_matrix(
                    image_paths,
                    labels,
                    feature_name,
                    image_size=self.image_size,
                    n_jobs=self.n_jobs,
                    show_progress=True,
                )
                self.feature_cache[feature_name] = payload
                LOGGER.info('   âœ“ ç»´åº¦ %d | æ ·æœ¬ %d', payload['dim'], len(payload['valid_indices']))
            except Exception as exc:
                LOGGER.warning('   âœ— è·³è¿‡ %s: %s', feature_name, exc)
        
        if not self.feature_cache:
            raise RuntimeError('æ²¡æœ‰æˆåŠŸæå–çš„ç‰¹å¾')
        
        return self.feature_cache
    
    def search(self, max_features=5):
        """æ‰§è¡ŒæŸæœç´¢ã€‚
        
        å‚æ•°:
            max_features: intï¼Œæœ€å¤§ç‰¹å¾ç»„åˆæ•°é‡
            
        è¿”å›:
            dict: æœç´¢ç»“æœï¼ŒåŒ…å« best, history, cache_hits
        """
        if not self.feature_cache:
            raise RuntimeError('è¯·å…ˆè°ƒç”¨ extract_features() æå–ç‰¹å¾')
        
        feature_names = list(self.feature_cache.keys())
        
        LOGGER.info('')
        LOGGER.info('=' * 60)
        LOGGER.info('å¼€å§‹ç‰¹å¾ç»„åˆæœç´¢ï¼ˆæŸæœç´¢ï¼‰')
        LOGGER.info('=' * 60)
        LOGGER.info('ğŸ” æŸæœç´¢ (Beam Search)')
        LOGGER.info('   ç‰¹å¾æ± å¤§å°: %d', len(feature_names))
        LOGGER.info('   æœ€å¤§ç»„åˆæ•°: %d', max_features)
        LOGGER.info('   æŸå®½: %d', self.beam_width)
        
        # åˆå§‹åŒ–ï¼šè¯„ä¼°æ‰€æœ‰å•ç‰¹å¾
        LOGGER.info('ğŸ“Š ç¬¬ 1 æ­¥: è¯„ä¼°å•ç‰¹å¾ç»„åˆ')
        candidates = []
        for name in feature_names:
            try:
                result = self._evaluate_subset([name])
                candidates.append(result)
                LOGGER.info('   â€¢ %s -> %.4f Â± %.4f', name, result['score'], result['std'])
            except Exception as exc:
                LOGGER.warning('   è¯„ä¼°å¤±è´¥ %s: %s', name, exc)
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œä¿ç•™ top beam_width
        candidates.sort(key=lambda x: x['score'], reverse=True)
        beam = candidates[:self.beam_width]
        best_overall = beam[0].copy() if beam else {'score': -1.0, 'subset': []}
        history = [{'step': 1, 'beam_size': len(beam), 'best_score': beam[0]['score'] if beam else -1.0}]
        
        LOGGER.info('   âœ… ä¿ç•™ top %d ä¸ªå€™é€‰', len(beam))
        for i, cand in enumerate(beam, 1):
            LOGGER.info('      %d. %.4f | %s', i, cand['score'], ' + '.join(cand['subset']))
        
        # é€æ­¥æ‰©å±•æŸ
        for step in range(2, max_features + 1):
            if not beam:
                break
            
            LOGGER.info('')
            LOGGER.info('ğŸ“Š ç¬¬ %d æ­¥: æ‰©å±•æŸ (å½“å‰æŸå¤§å°: %d)', step, len(beam))
            
            # ä»å½“å‰æŸä¸­çš„æ¯ä¸ªå€™é€‰ç”Ÿæˆæ–°å€™é€‰
            new_candidates = []
            used_combinations = set()
            
            for cand in beam:
                current_subset = set(cand['subset'])
                remaining = [name for name in feature_names if name not in current_subset]
                
                for new_feature in remaining:
                    new_subset = sorted(list(current_subset) + [new_feature])
                    subset_key = tuple(new_subset)
                    
                    # é¿å…é‡å¤è¯„ä¼°
                    if subset_key in used_combinations:
                        continue
                    used_combinations.add(subset_key)
                    
                    try:
                        result = self._evaluate_subset(new_subset)
                        new_candidates.append(result)
                        
                        LOGGER.info(
                            '   â€¢ %s -> %.4f Â± %.4f (%d ç»´%s)',
                            ' + '.join(new_subset),
                            result['score'],
                            result['std'],
                            result['original_dim'],
                            ' + PCA({})'.format(result['pca']) if result['pca'] else '',
                        )
                    except Exception as exc:
                        LOGGER.warning('   è¯„ä¼°å¤±è´¥ %s: %s', ' + '.join(new_subset), exc)
            
            if not new_candidates:
                LOGGER.warning('   æ²¡æœ‰æ–°å€™é€‰ï¼Œæå‰ç»“æŸ')
                break
            
            # æŒ‰å¾—åˆ†æ’åºï¼Œä¿ç•™ top beam_width
            new_candidates.sort(key=lambda x: x['score'], reverse=True)
            beam = new_candidates[:self.beam_width]
            
            # æ›´æ–°å…¨å±€æœ€ä½³
            if beam and beam[0]['score'] > best_overall['score']:
                best_overall = beam[0].copy()
            
            history.append({
                'step': step,
                'beam_size': len(beam),
                'best_score': beam[0]['score'] if beam else -1.0,
                'evaluated': len(new_candidates),
            })
            
            LOGGER.info('   âœ… ä¿ç•™ top %d ä¸ªå€™é€‰', len(beam))
            for i, cand in enumerate(beam, 1):
                LOGGER.info('      %d. %.4f Â± %.4f | %s', i, cand['score'], cand['std'], ' + '.join(cand['subset']))
        
        cache_hits = 0
        if self.evaluation_cache is not None:
            cache_hits = self.evaluation_cache.get('_hits', 0)
        
        LOGGER.info('')
        LOGGER.info('â­ æŸæœç´¢æœ€ä½³ç»„åˆ: %s', ' + '.join(best_overall['subset']))
        LOGGER.info('   å¾—åˆ†: %.4f Â± %.4f', best_overall['score'], best_overall.get('std', 0.0))
        LOGGER.info('   ç»´åº¦: %d%s', 
                    best_overall.get('original_dim', 0),
                    ' (PCA -> {})'.format(best_overall.get('pca', 0)) if best_overall.get('pca', 0) else '')
        if cache_hits > 0:
            LOGGER.info('   ç¼“å­˜å‘½ä¸­: %d æ¬¡', cache_hits)
        
        # ä¿å­˜ç»“æœåˆ°å®ä¾‹å˜é‡
        self.best_result = best_overall
        self.search_history = history
        
        return {
            'best': best_overall,
            'history': history,
            'cache_hits': cache_hits,
            'method': 'beam_search',
        }
    
    def save_best_features(self, project_root='.', output_dir='features'):
        """ä¿å­˜æœ€ä½³ç‰¹å¾ç»„åˆåˆ°æ–‡ä»¶ï¼ˆåˆ†åˆ«å¯¹ train/val/test æå–å¹¶ä¿å­˜ï¼‰ã€‚
        
        å‚æ•°:
            output_dir: str æˆ– Pathï¼Œè¾“å‡ºç›®å½•
            
        è¿”å›:
            dict æˆ– None: {split: Path} å­—å…¸ï¼›è‹¥æ— æœ€ä½³ç»„åˆæˆ–ç¼ºå°‘æ•°æ®é›†ä¿¡æ¯åˆ™è¿”å› None
            
        å¼‚å¸¸:
            RuntimeError: å½“ç¼ºå°‘æ•°æ®é›†ä¿¡æ¯æ—¶æŠ›å‡º
        """
        if self.best_result is None:
            LOGGER.warning('æœªæ‰¾åˆ°æœ€ä½³ç‰¹å¾ç»„åˆï¼Œè¯·å…ˆæ‰§è¡Œ search()')
            return None
        
        subset = self.best_result.get('subset', [])
        if not subset:
            LOGGER.warning('æœ€ä½³ç‰¹å¾ç»„åˆä¸ºç©º')
            return None
        
        # æ£€æŸ¥æ•°æ®é›†ä¿¡æ¯
        if self.dataset_dir is None:
            LOGGER.warning('æ— æ³•ä¿å­˜ç‰¹å¾ï¼šç¼ºå°‘æ•°æ®é›†ä¿¡æ¯')
            return None
        
        # ç»Ÿä¸€ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ features ç›®å½•
        output_dir = Path(project_root) / output_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # é‡ç½®PCAç›¸å…³çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self, '_scaler'):
            delattr(self, '_scaler')
        if hasattr(self, '_pca'):
            delattr(self, '_pca')
        
        LOGGER.info('âœ¨ æœ€ä½³ç»„åˆ: %s', ' + '.join(subset))
        LOGGER.info('ğŸ”„ åˆ†åˆ«åœ¨ train/val/test ä¸Šæå–å¹¶ä¿å­˜æœ€ä½³ç‰¹å¾ç»„åˆ...')
        
        saved_paths = {}
        for split_name in ['train', 'val', 'test']:
            # åŠ è½½å¯¹åº” split çš„å®Œæ•´æ•°æ®
            image_paths, labels = collect_image_paths_and_labels(self.dataset_dir, split_name)
            LOGGER.info('   [%s] æ•°æ®é›†å¤§å°: %d å¼ å›¾åƒ', split_name, len(image_paths))
            if not image_paths:
                LOGGER.warning('   [%s] æ— æ•°æ®ï¼Œè·³è¿‡ä¿å­˜', split_name)
                continue
            
            # æå–è¯¥ split çš„æœ€ä½³ç‰¹å¾ç»„åˆ
            payloads = []
            for feature_name in subset:
                LOGGER.info('   [%s] ğŸ”§ æå–ç‰¹å¾: %s', split_name, feature_name)
                try:
                    payload = UnifiedFeatureExtractor.extract_single_feature_matrix(
                        image_paths,
                        labels,
                        feature_name,
                        image_size=self.image_size,
                        n_jobs=self.n_jobs,
                        show_progress=True,
                    )
                    payloads.append(payload)
                    LOGGER.info('      [%s] âœ“ ç»´åº¦ %d | æ ·æœ¬ %d', split_name, payload['dim'], len(payload['valid_indices']))
                except Exception as exc:
                    LOGGER.error('      [%s] âœ— æå–å¤±è´¥ %s: %s', split_name, feature_name, exc)
                    raise RuntimeError('æ— æ³•åœ¨ {} æå–ç‰¹å¾: {}'.format(split_name, exc))
            
            X_split, y_split, indices = align_and_concatenate_features(payloads)
            LOGGER.info('   [%s] âœ… ç‰¹å¾æå–å®Œæˆ: %d æ ·æœ¬', split_name, len(indices))
            
            # å¦‚æœè¯„ä¼°æ—¶ä½¿ç”¨äº†PCAï¼Œéœ€è¦åœ¨ä¿å­˜æ—¶ä¹Ÿåº”ç”¨PCA
            original_dim = X_split.shape[1]
            pca_components = 0
            if self.pca_components is not None and original_dim > self.pca_components:
                pca_components = self.pca_components
            
            if pca_components > 0:
                # éœ€è¦åœ¨trainé›†ä¸Šæ‹ŸåˆPCAï¼Œç„¶ååº”ç”¨åˆ°æ‰€æœ‰split
                if split_name == 'train':
                    # åœ¨trainé›†ä¸Šæ‹Ÿåˆæ ‡å‡†åŒ–å™¨å’ŒPCA
                    scaler = StandardScaler()
                    X_split_scaled = scaler.fit_transform(X_split)
                    pca = PCA(n_components=pca_components, random_state=42)
                    X_split = pca.fit_transform(X_split_scaled)
                    # ä¿å­˜scalerå’Œpcaä¾›åç»­splitä½¿ç”¨
                    self._scaler = scaler
                    self._pca = pca
                    LOGGER.info('   [%s] ğŸ”§ åº”ç”¨PCA: %d -> %d ç»´', split_name, original_dim, pca_components)
                else:
                    # åœ¨val/testé›†ä¸Šä½¿ç”¨trainé›†æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨å’ŒPCA
                    if not hasattr(self, '_scaler') or not hasattr(self, '_pca'):
                        raise RuntimeError('PCAæœªåœ¨trainé›†ä¸Šæ‹Ÿåˆï¼Œæ— æ³•åº”ç”¨åˆ°{}é›†'.format(split_name))
                    X_split_scaled = self._scaler.transform(X_split)
                    X_split = self._pca.transform(X_split_scaled)
                    LOGGER.info('   [%s] ğŸ”§ åº”ç”¨PCA: %d -> %d ç»´', split_name, original_dim, pca_components)
            
            feature_info = {
                'selected_features': subset,
                'original_dim': self.best_result.get('original_dim'),
                'pca': pca_components,
                'score': self.best_result.get('score'),
                'std': self.best_result.get('std'),
                'split': split_name,
                'use_full_dataset': True,
                'n_samples': len(indices),
            }
            split_features_path = output_dir / f'{split_name}_features.joblib'
            save_features_to_file(X_split, y_split, indices, feature_info, split_features_path)
            LOGGER.info('ğŸ’¾ [%s] ç‰¹å¾å·²ä¿å­˜: %s', split_name, split_features_path)
            saved_paths[split_name] = split_features_path
        
        if not saved_paths:
            return None
        return saved_paths
    
    def save_search_results(self, output_dir='runs/feature_search'):
        """ä¿å­˜æœç´¢ç»“æœåˆ° JSON æ–‡ä»¶ã€‚
        
        å‚æ•°:
            output_dir: str æˆ– Pathï¼Œè¾“å‡ºç›®å½•
            
        è¿”å›:
            Path: ä¿å­˜çš„ JSON æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°ç»“æœåˆ™è¿”å› None
        """
        if self.best_result is None:
            LOGGER.warning('æœªæ‰¾åˆ°æœç´¢ç»“æœï¼Œè¯·å…ˆæ‰§è¡Œ search()')
            return None
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        results_data = {
            'timestamp': datetime.now().isoformat(),
            'config': {
                'model': self.model_name,
                'cv': self.cv,
                'scoring': self.scoring,
                'beam_width': self.beam_width,
                'enable_cache': self.enable_cache,
                'image_size': self.image_size,
                'n_jobs': self.n_jobs,
                'pca_components': self.pca_components,
                'dataset_dir': str(self.dataset_dir) if self.dataset_dir else None,
                'split': self.split,
                'sample_ratio': self.sample_ratio,
            },
            'best_result': {
                'subset': self.best_result.get('subset', []),
                'score': self.best_result.get('score', 0.0),
                'std': self.best_result.get('std', 0.0),
                'original_dim': self.best_result.get('original_dim', 0),
                'pca': self.best_result.get('pca', 0),
            },
            'search_history': self.search_history,
            'feature_cache_info': {
                name: {
                    'dim': payload.get('dim', 0),
                    'n_samples': len(payload.get('valid_indices', [])),
                }
                for name, payload in self.feature_cache.items()
            },
        }
        
        if self.evaluation_cache is not None:
            cache_hits = self.evaluation_cache.get('_hits', 0)
            results_data['cache_info'] = {
                'hits': cache_hits,
                'size': len([k for k in self.evaluation_cache.keys() if k != '_hits']),
            }
        
        results_path = output_dir / 'search_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        LOGGER.info('ğŸ“ æœç´¢ç»“æœå·²ä¿å­˜: %s', results_path)
        return results_path
    
    def run(self, dataset_dir='dataset', split='train', sample_ratio=1.0, 
            max_features=5, output_dir='runs/feature_search', feature_names=None,
            save_features=True, save_log=True):
        """è¿è¡Œå®Œæ•´çš„ç‰¹å¾æœç´¢æµç¨‹ã€‚
        
        å‚æ•°:
            dataset_dir: strï¼Œæ•°æ®é›†æ ¹ç›®å½•
            split: strï¼Œæ•°æ®é›†åˆ†å‰² (train/val/test)
            sample_ratio: floatï¼Œé‡‡æ ·æ¯”ä¾‹ (0-1]
            max_features: intï¼Œæœ€å¤§ç‰¹å¾ç»„åˆæ•°é‡
            output_dir: str æˆ– Pathï¼Œè¾“å‡ºç›®å½•
            feature_names: list æˆ– Noneï¼Œè¦æå–çš„ç‰¹å¾åç§°åˆ—è¡¨
            save_features: boolï¼Œæ˜¯å¦ä¿å­˜æœ€ä½³ç‰¹å¾ï¼ˆä½¿ç”¨å®Œæ•´æ•°æ®é›†æå–ï¼‰
            save_log: boolï¼Œæ˜¯å¦ä¿å­˜æ—¥å¿—æ–‡ä»¶
            
        è¿”å›:
            dict: æœç´¢æ‘˜è¦ä¿¡æ¯
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶
        if save_log:
            log_file = output_dir / 'feature_search.log'
            # å°†æ–‡ä»¶æ—¥å¿—å™¨æ·»åŠ åˆ°ä¸»æ—¥å¿—å™¨
            if not any(isinstance(h, logging.FileHandler) for h in LOGGER.handlers):
                file_handler = logging.FileHandler(str(log_file), encoding='utf-8')
                file_handler.setFormatter(logging.Formatter(
                    '%(asctime)s | %(name)s | %(levelname)s | %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S'
                ))
                LOGGER.addHandler(file_handler)
            LOGGER.info('ğŸ“ æ—¥å¿—å°†ä¿å­˜åˆ°: %s', log_file)
        
        # åŠ è½½æ•°æ®
        image_paths, labels = self.load_data(dataset_dir, split, sample_ratio)
        
        # æå–ç‰¹å¾
        self.extract_features(image_paths, labels, feature_names)
        
        # æ‰§è¡Œæœç´¢
        search_result = self.search(max_features=max_features)
        
        # ä¿å­˜æœç´¢ç»“æœåˆ° JSON
        results_path = self.save_search_results(str(output_dir))
        
        # ä¿å­˜æœ€ä½³ç‰¹å¾ï¼ˆä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œå¦‚æœå¯ç”¨ï¼‰
        best_features_path = None
        if save_features:
            saved = self.save_best_features(project_root=str(project_root), output_dir='features')
            if isinstance(saved, dict):
                best_features_path = {k: str(v) for k, v in saved.items()}
            else:
                best_features_path = None
        else:
            LOGGER.info('â­ï¸  è·³è¿‡ç‰¹å¾ä¿å­˜ï¼ˆä½¿ç”¨ --no_save_features æ—¶ï¼‰')
        
        best = search_result.get('best', {})
        subset = best.get('subset', [])
        
        if not subset:
            LOGGER.info('æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„ç‰¹å¾ç»„åˆ')
        
        return {
            'best_subset': subset,
            'best_score': best.get('score', 0.0),
            'best_std': best.get('std', 0.0),
            'output_dir': str(output_dir),
            'best_features_path': best_features_path if best_features_path else None,
            'results_path': str(results_path) if results_path else None,
            'search_method': 'beam_search',
            'search_history': search_result.get('history', []),
            'cache_hits': search_result.get('cache_hits', 0),
        }


def build_arg_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚"""
    parser = argparse.ArgumentParser(
        description='ç‰¹å¾ç»„åˆæœç´¢ï¼ˆæŸæœç´¢ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--dataset_dir', default='dataset', help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--image_size', type=int, default=128, help='å›¾åƒç¼©æ”¾å°ºå¯¸')
    parser.add_argument('--model', default='svm', choices=['svm', 'rf'], help='è¯„ä¼°æ¨¡å‹')
    parser.add_argument('--cv', type=int, default=5, help='äº¤å‰éªŒè¯æŠ˜æ•°')
    parser.add_argument('--sample_ratio', type=float, default=1.0, help='æ•°æ®é‡‡æ ·æ¯”ä¾‹ (0-1]')
    parser.add_argument('--max_features', type=int, default=8, help='æœ€å¤§é€‰æ‹©ç‰¹å¾æ•°é‡')
    parser.add_argument('--scoring', default='accuracy', help='è¯„åˆ†æŒ‡æ ‡')
    parser.add_argument('--out_dir', default='runs/feature_search', help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--n_jobs', type=int, default=16, help='ç‰¹å¾æå–å¹¶è¡Œçº¿ç¨‹æ•°')
    parser.add_argument('--beam_width', type=int, default=5, help='æŸæœç´¢çš„æŸå®½')
    parser.add_argument('--pca_components', type=int, default=512, help='PCAé™ç»´åçš„ç»´åº¦ï¼ˆè®¾ç½®ä¸º0æˆ–è´Ÿæ•°è¡¨ç¤ºä¸é™ç»´ï¼‰')
    parser.add_argument('--enable_cache', action='store_true', help='å¯ç”¨è¯„ä¼°ç»“æœç¼“å­˜')
    parser.add_argument('--no_save_features', action='store_true', help='ä¸ä¿å­˜æœ€ä½³ç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--no_save_log', action='store_true', help='ä¸ä¿å­˜æ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--log_level', default='INFO', help='æ—¥å¿—çº§åˆ«')
    return parser


def parse_args(args=None):
    """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
    parser = build_arg_parser()
    return parser.parse_args(args=args)


def main():
    """å‘½ä»¤è¡Œå…¥å£ã€‚"""
    args = parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',
    )
    
    # æ˜¾ç¤ºæœç´¢é…ç½®
    LOGGER.info('=' * 60)
    LOGGER.info('ç‰¹å¾æœç´¢é…ç½®ï¼ˆæŸæœç´¢ï¼‰')
    LOGGER.info('=' * 60)
    LOGGER.info('æ•°æ®é›†: %s', args.dataset_dir)
    LOGGER.info('é‡‡æ ·æ¯”ä¾‹: %.0f%%', args.sample_ratio * 100)
    LOGGER.info('æœ€å¤§ç‰¹å¾æ•°: %d', args.max_features)
    LOGGER.info('æŸå®½: %d', args.beam_width)
    LOGGER.info('è¯„ä¼°æ¨¡å‹: %s', args.model)
    LOGGER.info('äº¤å‰éªŒè¯: %d æŠ˜', args.cv)
    pca_components = args.pca_components if args.pca_components > 0 else None
    if pca_components is not None:
        LOGGER.info('PCAé™ç»´: %d ç»´', pca_components)
    else:
        LOGGER.info('PCAé™ç»´: ç¦ç”¨')
    LOGGER.info('=' * 60)
    LOGGER.info('')
    
    # åˆ›å»ºæœç´¢å™¨å¹¶è¿è¡Œ
    searcher = FeatureSearch(
        model=args.model,
        cv=args.cv,
        scoring=args.scoring,
        beam_width=args.beam_width,
        enable_cache=args.enable_cache,
        image_size=args.image_size,
        n_jobs=args.n_jobs,
        pca_components=pca_components,
    )
    
    summary = searcher.run(
        dataset_dir=args.dataset_dir,
        split='train',
        sample_ratio=args.sample_ratio,
        max_features=args.max_features,
        output_dir=args.out_dir,
        save_features=not args.no_save_features,
        save_log=not args.no_save_log,
    )
    
    LOGGER.info('')
    LOGGER.info('=' * 60)
    LOGGER.info('æœç´¢å®Œæˆ')
    LOGGER.info('=' * 60)
    LOGGER.info('æœ€ä½³ç»„åˆ: %s', ' + '.join(summary['best_subset']))
    LOGGER.info('æœ€ä½³å¾—åˆ†: %.4f Â± %.4f', summary['best_score'], summary['best_std'])
    if summary.get('cache_hits', 0) > 0:
        LOGGER.info('ç¼“å­˜å‘½ä¸­: %d æ¬¡', summary['cache_hits'])
    LOGGER.info('ç»“æœç›®å½•: %s', summary['output_dir'])
    if summary.get('results_path'):
        LOGGER.info('æœç´¢ç»“æœ: %s', summary['results_path'])
    if summary.get('best_features_path'):
        LOGGER.info('æœ€ä½³ç‰¹å¾: %s', summary['best_features_path'])
    LOGGER.info('=' * 60)


if __name__ == '__main__':
    main()
