"""
çŒ«ç‹—åˆ†ç±» - BPç¥ç»ç½‘ç»œæ¨¡å‹

è¯¥æ¨¡å—å®ç°äº†ç»å…¸çš„BPï¼ˆBack Propagationï¼‰ç¥ç»ç½‘ç»œã€‚
BPç¥ç»ç½‘ç»œæ˜¯æœ€åŸºç¡€çš„å¤šå±‚æ„ŸçŸ¥æœºï¼Œé€šè¿‡åå‘ä¼ æ’­ç®—æ³•è®­ç»ƒã€‚

ä¸»è¦åŠŸèƒ½:
    - æ”¯æŒå¤šéšè—å±‚çš„å…¨è¿æ¥ç½‘ç»œ
    - å¯é…ç½®çš„æ¿€æ´»å‡½æ•°ï¼ˆReLU, Sigmoid, Tanhï¼‰
    - å¤šç§ä¼˜åŒ–å™¨æ”¯æŒï¼ˆSGD, Adam, RMSpropï¼‰
    - Dropoutæ­£åˆ™åŒ–å’Œæ‰¹å½’ä¸€åŒ–
    - å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from pathlib import Path
import json
from tqdm import tqdm


class BPNeuralNetwork(nn.Module):
    """BPç¥ç»ç½‘ç»œåˆ†ç±»å™¨
    
    ç»å…¸çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•è®­ç»ƒã€‚
    é€‚åˆå¤„ç†æ‰å¹³åŒ–çš„å›¾åƒç‰¹å¾æˆ–å…¶ä»–ç»“æ„åŒ–æ•°æ®ã€‚
    
    å‚æ•°:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_sizes: éšè—å±‚å¤§å°åˆ—è¡¨ï¼Œå¦‚[512, 256, 128]è¡¨ç¤º3ä¸ªéšè—å±‚
        num_classes: è¾“å‡ºç±»åˆ«æ•°ï¼ˆäºŒåˆ†ç±»é€šå¸¸ä¸º1ï¼‰
        activation: æ¿€æ´»å‡½æ•°ç±»å‹ ('relu', 'sigmoid', 'tanh', 'leaky_relu')
        dropout_p: Dropoutæ¦‚ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
        use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
        bias: æ˜¯å¦ä½¿ç”¨åç½®é¡¹
    """
    
    def __init__(self, input_size, hidden_sizes=[512, 256, 128], num_classes=1, 
                 activation='relu', dropout_p=0.5, use_batch_norm=True, bias=True):
        super().__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_classes = num_classes
        self.activation_name = activation
        self.dropout_p = dropout_p
        self.use_batch_norm = use_batch_norm
        
        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        self.activation = self._get_activation_function(activation)
        
        # æ„å»ºç½‘ç»œå±‚
        self.layers = self._build_layers(input_size, hidden_sizes, num_classes, bias)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _get_activation_function(self, activation):
        """è·å–æ¿€æ´»å‡½æ•°
        
        å‚æ•°:
            activation: æ¿€æ´»å‡½æ•°åç§°
            
        è¿”å›:
            PyTorchæ¿€æ´»å‡½æ•°
        """
        activations = {
            'relu': nn.ReLU(inplace=True),
            'sigmoid': nn.Sigmoid(),
            'tanh': nn.Tanh(),
            'leaky_relu': nn.LeakyReLU(0.2, inplace=True),
            'elu': nn.ELU(inplace=True),
            'gelu': nn.GELU()
        }
        
        if activation not in activations:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {activation}ã€‚å¯é€‰: {list(activations.keys())}")
        
        return activations[activation]
    
    def _build_layers(self, input_size, hidden_sizes, num_classes, bias):
        """æ„å»ºç½‘ç»œå±‚
        
        å‚æ•°:
            input_size: è¾“å…¥ç»´åº¦
            hidden_sizes: éšè—å±‚å¤§å°åˆ—è¡¨
            num_classes: è¾“å‡ºç±»åˆ«æ•°
            bias: æ˜¯å¦ä½¿ç”¨åç½®
            
        è¿”å›:
            ç½‘ç»œå±‚çš„ModuleList
        """
        layers = nn.ModuleList()
        
        # æ‰€æœ‰å±‚çš„å¤§å°
        all_sizes = [input_size] + hidden_sizes + [num_classes]
        
        # æ„å»ºéšè—å±‚
        for i in range(len(all_sizes) - 1):
            in_features = all_sizes[i]
            out_features = all_sizes[i + 1]
            is_output_layer = (i == len(all_sizes) - 2)
            
            # çº¿æ€§å±‚
            linear = nn.Linear(in_features, out_features, bias=bias)
            layers.append(linear)
            
            # éè¾“å‡ºå±‚æ·»åŠ æ¿€æ´»å‡½æ•°ã€æ‰¹å½’ä¸€åŒ–å’ŒDropout
            if not is_output_layer:
                # æ‰¹å½’ä¸€åŒ–ï¼ˆåœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ï¼‰
                if self.use_batch_norm:
                    layers.append(nn.BatchNorm1d(out_features))
                
                # æ¿€æ´»å‡½æ•°
                layers.append(self.activation)
                
                # Dropout
                if self.dropout_p > 0:
                    layers.append(nn.Dropout(p=self.dropout_p))
        
        return layers
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡
        
        ä½¿ç”¨Xavier/Glorotåˆå§‹åŒ–æ¥ä¿æŒæ¢¯åº¦ç¨³å®š
        """
        for layer in self.modules():
            if isinstance(layer, nn.Linear):
                # Xavieråˆå§‹åŒ–
                if self.activation_name == 'relu':
                    # Heåˆå§‹åŒ–æ›´é€‚åˆReLU
                    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')
                else:
                    nn.init.xavier_uniform_(layer.weight)
                
                if layer.bias is not None:
                    nn.init.zeros_(layer.bias)
            elif isinstance(layer, nn.BatchNorm1d):
                nn.init.ones_(layer.weight)
                nn.init.zeros_(layer.bias)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­
        
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (N, input_size)
               N: batch size
               input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        
        è¿”å›:
            è¾“å‡ºlogitsï¼Œå½¢çŠ¶ä¸º (N, num_classes)
        """
        # ç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if x.dim() > 2:
            x = x.view(x.size(0), -1)  # å±•å¹³ä¸ºäºŒç»´
        
        # é€šè¿‡æ‰€æœ‰å±‚
        for layer in self.layers:
            x = layer(x)
        
        return x
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯
        
        è¿”å›:
            åŒ…å«æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'BP Neural Network',
            'input_size': self.input_size,
            'hidden_sizes': self.hidden_sizes,
            'num_classes': self.num_classes,
            'activation': self.activation_name,
            'dropout_p': self.dropout_p,
            'use_batch_norm': self.use_batch_norm,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'num_layers': len(self.hidden_sizes) + 1
        }


class BPNeuralNetworkTrainer:
    """BPç¥ç»ç½‘ç»œè®­ç»ƒå™¨
    
    å°è£…äº†BPç¥ç»ç½‘ç»œçš„è®­ç»ƒã€è¯„ä¼°å’Œä¿å­˜åŠŸèƒ½ã€‚
    æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥å’Œè®­ç»ƒæŠ€å·§ã€‚
    
    å‚æ•°:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_sizes: éšè—å±‚é…ç½®
        num_classes: è¾“å‡ºç±»åˆ«æ•°
        activation: æ¿€æ´»å‡½æ•°
        dropout_p: Dropoutæ¦‚ç‡
        use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
        device: è®­ç»ƒè®¾å¤‡
    """
    
    def __init__(self, input_size, hidden_sizes=[512, 256, 128], num_classes=1,
                 activation='relu', dropout_p=0.5, use_batch_norm=True, device='cuda'):
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_classes = num_classes
        self.activation = activation
        self.dropout_p = dropout_p
        self.use_batch_norm = use_batch_norm
        self.device = device
        
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None
        self.training_history = []
    
    def build_model(self):
        """æ„å»ºBPç¥ç»ç½‘ç»œæ¨¡å‹
        
        è¿”å›:
            æ„å»ºå¥½çš„æ¨¡å‹
        """
        self.model = BPNeuralNetwork(
            input_size=self.input_size,
            hidden_sizes=self.hidden_sizes,
            num_classes=self.num_classes,
            activation=self.activation,
            dropout_p=self.dropout_p,
            use_batch_norm=self.use_batch_norm
        )
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.model = self.model.to(self.device)
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        info = self.model.get_model_info()
        print(f"ğŸ§  æ„å»ºBPç¥ç»ç½‘ç»œ:")
        print(f"   æ¶æ„: {self.input_size} â†’ {' â†’ '.join(map(str, self.hidden_sizes))} â†’ {self.num_classes}")
        print(f"   æ¿€æ´»å‡½æ•°: {info['activation']}")
        print(f"   æ€»å‚æ•°: {info['total_params']:,}")
        print(f"   å±‚æ•°: {info['num_layers']}")
        print(f"   æ‰¹å½’ä¸€åŒ–: {'æ˜¯' if self.use_batch_norm else 'å¦'}")
        print(f"   Dropout: {self.dropout_p}")
        
        return self.model
    
    def setup_training(self, optimizer='adam', learning_rate=1e-3, weight_decay=1e-4,
                      scheduler_type='step', scheduler_params=None):
        """è®¾ç½®è®­ç»ƒç»„ä»¶
        
        å‚æ•°:
            optimizer: ä¼˜åŒ–å™¨ç±»å‹ ('adam', 'sgd', 'rmsprop', 'adamw')
            learning_rate: åˆå§‹å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
            scheduler_type: å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ ('step', 'cosine', 'plateau', None)
            scheduler_params: è°ƒåº¦å™¨å‚æ•°å­—å…¸
        """
        if self.model is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨build_model()æ„å»ºæ¨¡å‹")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizers = {
            'adam': optim.Adam,
            'adamw': optim.AdamW,
            'sgd': optim.SGD,
            'rmsprop': optim.RMSprop
        }
        
        if optimizer not in optimizers:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer}")
        
        optimizer_class = optimizers[optimizer]
        
        if optimizer == 'sgd':
            self.optimizer = optimizer_class(
                self.model.parameters(), 
                lr=learning_rate, 
                weight_decay=weight_decay,
                momentum=0.9  # SGDé€šå¸¸éœ€è¦åŠ¨é‡
            )
        else:
            self.optimizer = optimizer_class(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        if self.num_classes == 1:
            self.criterion = nn.BCEWithLogitsLoss()
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        if scheduler_type is not None:
            scheduler_params = scheduler_params or {}
            
            if scheduler_type == 'step':
                self.scheduler = optim.lr_scheduler.StepLR(
                    self.optimizer,
                    step_size=scheduler_params.get('step_size', 10),
                    gamma=scheduler_params.get('gamma', 0.1)
                )
            elif scheduler_type == 'cosine':
                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer,
                    T_max=scheduler_params.get('T_max', 50)
                )
            elif scheduler_type == 'plateau':
                self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizer,
                    mode='max',  # ç›‘æ§å‡†ç¡®ç‡
                    factor=scheduler_params.get('factor', 0.5),
                    patience=scheduler_params.get('patience', 5),
                    verbose=True
                )
        
        print(f"âš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"   ä¼˜åŒ–å™¨: {optimizer}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   æƒé‡è¡°å‡: {weight_decay}")
        print(f"   è°ƒåº¦å™¨: {scheduler_type}")
        print(f"   æŸå¤±å‡½æ•°: {'BCEWithLogitsLoss' if self.num_classes == 1 else 'CrossEntropyLoss'}")
    
    def train_epoch(self, train_loader, epoch, total_epochs):
        """è®­ç»ƒä¸€ä¸ªepoch
        
        å‚æ•°:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epochæ•°
            total_epochs: æ€»epochæ•°
            
        è¿”å›:
            epochè®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
        """
        if self.model is None or self.optimizer is None or self.criterion is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨build_model()å’Œsetup_training()")
            
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        # è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{total_epochs}")
        
        for batch_idx, (data, targets) in enumerate(pbar):
            data = data.to(self.device)
            targets = targets.to(self.device)
            
            # å±•å¹³è¾“å…¥æ•°æ®ï¼ˆå¦‚æœæ˜¯å›¾åƒï¼‰
            if data.dim() > 2:
                data = data.view(data.size(0), -1)
            
            # ç¡®ä¿ç›®æ ‡æ ¼å¼æ­£ç¡®
            if self.num_classes == 1:
                targets = targets.float().view(-1, 1)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            
            if self.num_classes == 1:
                # äºŒåˆ†ç±»
                predicted = (torch.sigmoid(outputs) > 0.5).float()
                correct += (predicted == targets).sum().item()
            else:
                # å¤šåˆ†ç±»
                predicted = outputs.argmax(dim=1)
                correct += (predicted == targets).sum().item()
            
            total += targets.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_lr = self.optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'LR': f'{current_lr:.6f}'
            })
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def evaluate(self, test_loader, dataset_name="Test"):
        """è¯„ä¼°æ¨¡å‹
        
        å‚æ•°:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            dataset_name: æ•°æ®é›†åç§°
            
        è¿”å›:
            æµ‹è¯•æŸå¤±å’Œå‡†ç¡®ç‡
        """
        if self.model is None or self.criterion is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨build_model()å’Œsetup_training()")
            
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, targets in test_loader:
                data = data.to(self.device)
                targets = targets.to(self.device)
                
                # å±•å¹³è¾“å…¥æ•°æ®
                if data.dim() > 2:
                    data = data.view(data.size(0), -1)
                
                # ç¡®ä¿ç›®æ ‡æ ¼å¼æ­£ç¡®
                if self.num_classes == 1:
                    targets = targets.float().view(-1, 1)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                
                if self.num_classes == 1:
                    # äºŒåˆ†ç±»
                    predicted = (torch.sigmoid(outputs) > 0.5).float()
                    correct += (predicted == targets).sum().item()
                else:
                    # å¤šåˆ†ç±»
                    predicted = outputs.argmax(dim=1)
                    correct += (predicted == targets).sum().item()
                
                total += targets.size(0)
        
        avg_loss = total_loss / len(test_loader)
        accuracy = 100. * correct / total
        
        print(f"\nğŸ“Š {dataset_name} ç»“æœ:")
        print(f"   æŸå¤±: {avg_loss:.4f}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader=None, epochs=100, early_stopping_patience=10, save_path=None):
        """è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        è¿”å›:
            è®­ç»ƒå†å²
        """
        if self.model is None or self.optimizer is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨build_model()å’Œsetup_training()")
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒBPç¥ç»ç½‘ç»œ ({epochs} epochs)")
        
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, epoch, epochs)
            
            # éªŒè¯
            val_loss, val_acc = 0.0, 0.0
            if val_loader is not None:
                val_loss, val_acc = self.evaluate(val_loader, "Validation")
            
            # è®°å½•å†å²
            epoch_history = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'lr': self.optimizer.param_groups[0]['lr']
            }
            self.training_history.append(epoch_history)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_acc if val_loader else train_acc)
                else:
                    self.scheduler.step()
            
            # æ—©åœæ£€æŸ¥
            if val_loader is not None:
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if save_path:
                        self.save_model(save_path, epoch_history)
                else:
                    patience_counter += 1
                
                if patience_counter >= early_stopping_patience:
                    print(f"ğŸ›‘ æ—©åœè§¦å‘ (patience={early_stopping_patience})")
                    break
            
            print(f"Epoch {epoch+1:3d}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%")
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        return self.training_history
    
    def save_model(self, save_path, training_info=None):
        """ä¿å­˜æ¨¡å‹
        
        å‚æ•°:
            save_path: ä¿å­˜è·¯å¾„
            training_info: è®­ç»ƒä¿¡æ¯
        """
        if self.model is None:
            raise RuntimeError("æ²¡æœ‰æ¨¡å‹å¯ä¿å­˜")
        
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®
        model_data = {
            'model_state_dict': self.model.state_dict(),
            'model_config': self.model.get_model_info(),
            'training_history': self.training_history,
            'training_info': training_info or {}
        }
        
        torch.save(model_data, save_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")


def create_bp_network(input_size, hidden_sizes=[512, 256, 128], num_classes=1, activation='relu'):
    """åˆ›å»ºBPç¥ç»ç½‘ç»œçš„ä¾¿æ·å‡½æ•°
    
    å‚æ•°:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_sizes: éšè—å±‚é…ç½®
        num_classes: è¾“å‡ºç±»åˆ«æ•°
        activation: æ¿€æ´»å‡½æ•°
        
    è¿”å›:
        BPNeuralNetworkå®ä¾‹
    """
    return BPNeuralNetwork(input_size, hidden_sizes, num_classes, activation)


def create_bp_trainer(input_size, **kwargs):
    """åˆ›å»ºBPç¥ç»ç½‘ç»œè®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°
    
    å‚æ•°:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        **kwargs: BPNeuralNetworkTrainerçš„å…¶ä»–å‚æ•°
        
    è¿”å›:
        BPNeuralNetworkTrainerå®ä¾‹
        
    ç¤ºä¾‹:
        >>> # ä¸º224x224çš„RGBå›¾åƒåˆ›å»ºBPç½‘ç»œ
        >>> trainer = create_bp_trainer(
        ...     input_size=224*224*3,  # å±•å¹³åçš„å›¾åƒå°ºå¯¸
        ...     hidden_sizes=[1024, 512, 256],
        ...     activation='relu',
        ...     dropout_p=0.5
        ... )
        >>> 
        >>> # æ„å»ºå’Œè®¾ç½®è®­ç»ƒ
        >>> model = trainer.build_model()
        >>> trainer.setup_training(optimizer='adam', learning_rate=1e-3)
        >>> 
        >>> # å¼€å§‹è®­ç»ƒ
        >>> history = trainer.train(train_loader, val_loader, epochs=50)
    """
    return BPNeuralNetworkTrainer(input_size=input_size, **kwargs)


def load_bp_model(model_path, input_size):
    """åŠ è½½ä¿å­˜çš„BPç¥ç»ç½‘ç»œæ¨¡å‹
    
    å‚æ•°:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        
    è¿”å›:
        åŠ è½½å¥½çš„æ¨¡å‹
    """
    model_data = torch.load(model_path)
    model_config = model_data['model_config']
    
    # é‡å»ºæ¨¡å‹
    model = BPNeuralNetwork(
        input_size=input_size,
        hidden_sizes=model_config['hidden_sizes'],
        num_classes=model_config['num_classes'],
        activation=model_config['activation'],
        dropout_p=model_config['dropout_p'],
        use_batch_norm=model_config['use_batch_norm']
    )
    
    # åŠ è½½æƒé‡
    model.load_state_dict(model_data['model_state_dict'])
    
    print(f"ğŸ“‚ å·²åŠ è½½BPç¥ç»ç½‘ç»œæ¨¡å‹: {model_path}")
    return model, model_data
