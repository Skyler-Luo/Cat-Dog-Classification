"""
çŒ«ç‹—åˆ†ç±» - é€»è¾‘å›å½’æ¨¡å‹

è¯¥æ¨¡å—å®ç°äº†åŸºäºscikit-learnçš„é€»è¾‘å›å½’åˆ†ç±»å™¨ã€‚
é€»è¾‘å›å½’æ˜¯ä¸€ç§ç®€å•ä½†é«˜æ•ˆçš„çº¿æ€§åˆ†ç±»ç®—æ³•ï¼Œé€‚åˆä½œä¸ºåŸºçº¿æ¨¡å‹ã€‚

ä¸»è¦åŠŸèƒ½:
    - æ”¯æŒå¤šç§æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆL1, L2, ElasticNetï¼‰
    - è‡ªåŠ¨è¶…å‚æ•°æœç´¢ï¼ˆç½‘æ ¼æœç´¢ï¼‰
    - æ¨¡å‹ç³»æ•°åˆ†æ
"""
import numpy as np

from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.linear_model import LogisticRegression

from src.utils.ml_training import compute_classification_metrics, save_sklearn_model
from src.utils.logger import Logger


class LogisticRegressionTrainer:
    """é€»è¾‘å›å½’è®­ç»ƒå™¨ç±»
    
    å°è£…äº†é€»è¾‘å›å½’çš„è®­ç»ƒã€è¶…å‚æ•°æœç´¢ã€è¯„ä¼°å’Œä¿å­˜åŠŸèƒ½ã€‚
    æ”¯æŒå¤šç§æ­£åˆ™åŒ–ç­–ç•¥ã€‚
    
    å‚æ•°:
        C_values: æ­£åˆ™åŒ–å¼ºåº¦çš„å€™é€‰å€¼åˆ—è¡¨ï¼ˆè¶Šå°æ­£åˆ™åŒ–è¶Šå¼ºï¼‰
        penalty_types: æ­£åˆ™åŒ–ç±»å‹ ('l1', 'l2', 'elasticnet', 'none')
        l1_ratios: ElasticNetçš„L1æ¯”ä¾‹ï¼ˆä»…å½“penalty='elasticnet'æ—¶ä½¿ç”¨ï¼‰
        max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        cv_folds: äº¤å‰éªŒè¯çš„æŠ˜æ•°
        n_jobs: å¹¶è¡Œè®­ç»ƒçš„ä½œä¸šæ•°
        random_state: éšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°
        solvers: æ±‚è§£å™¨å€™é€‰åˆ—è¡¨ ('liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga')
    """
    
    def __init__(
        self,
        C_values=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
        penalty_types=['l1', 'l2'],
        l1_ratios=[0.1, 0.5, 0.7, 0.9],
        max_iter=1000,
        cv_folds=5,
        n_jobs=4,
        random_state=42,
        solvers=None,
        scoring='accuracy',
        do_search=True,
        default_params=None,
    ):
        self.C_values = C_values
        self.penalty_types = penalty_types
        self.l1_ratios = l1_ratios
        self.max_iter = max_iter
        self.cv_folds = cv_folds
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.best_model = None
        self.feature_names = None
        self.scoring = scoring
        self.do_search = do_search
        self.logger = None
        self.default_params = default_params or {}
        self.solvers = solvers or ['liblinear']
        if self.default_params.get('solver') and self.default_params['solver'] not in self.solvers:
            self.solvers.append(self.default_params['solver'])
        self.solvers = list(dict.fromkeys(self.solvers))
        self.valid_solver_penalty_pairs = []
        self.default_solver = None
        self.cv_results_ = None
        
        # éªŒè¯solverå’Œpenaltyçš„å…¼å®¹æ€§
        self._validate_solver_penalty()
        if not self.default_solver:
            candidate_solver = self.default_params.get('solver')
            if candidate_solver and any(solver == candidate_solver for solver, _ in self.valid_solver_penalty_pairs):
                self.default_solver = candidate_solver
        if not self.default_solver and self.valid_solver_penalty_pairs:
            self.default_solver = self.valid_solver_penalty_pairs[0][0]
            self.default_params['solver'] = self.default_solver
    
    def _get_logger(self):
        """è·å–loggerï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„"""
        if self.logger is None:
            self.logger = Logger(name="logistic_regression_trainer")
        return self.logger

    def _validate_solver_penalty(self):
        """éªŒè¯æ±‚è§£å™¨å’Œæ­£åˆ™åŒ–ç±»å‹çš„å…¼å®¹æ€§"""
        incompatible = []
        valid = []
        
        for solver in self.solvers:
            for penalty in self.penalty_types:
                if penalty == 'elasticnet' and solver != 'saga':
                    incompatible.append((solver, penalty))
                    continue
                if penalty == 'l1' and solver not in ['liblinear', 'saga']:
                    incompatible.append((solver, penalty))
                    continue
                if penalty == 'none' and solver not in ['newton-cg', 'lbfgs', 'sag', 'saga']:
                    incompatible.append((solver, penalty))
                    continue
                if solver == 'liblinear' and penalty == 'none':
                    incompatible.append((solver, penalty))
                    continue
                valid.append((solver, penalty))
        
        if not valid:
            raise ValueError("æœªæ‰¾åˆ°åˆæ³•çš„ solver-penalty ç»„åˆï¼Œè¯·è°ƒæ•´å‚æ•°ã€‚")
        
        seen = set()
        for solver, penalty in valid:
            if (solver, penalty) not in seen:
                self.valid_solver_penalty_pairs.append((solver, penalty))
                seen.add((solver, penalty))
        if incompatible:
            logger = self._get_logger()
            msg = "âš ï¸  æ£€æµ‹åˆ°ä¸å…¼å®¹çš„solver-penaltyç»„åˆ:"
            logger.info(msg)
            for solver, penalty in incompatible:
                logger.info(f"   {solver} + {penalty}")
        if self.default_params.get('penalty') and self.valid_solver_penalty_pairs:
            for solver, penalty in self.valid_solver_penalty_pairs:
                if self.default_params['penalty'] == penalty:
                    self.default_solver = solver
                    break
        if not self.default_solver and self.default_params.get('solver'):
            candidate_solver = self.default_params['solver']
            if any(solver == candidate_solver for solver, _ in self.valid_solver_penalty_pairs):
                self.default_solver = candidate_solver

    def _build_model(self, **params):
        """æ„å»ºé€»è¾‘å›å½’åˆ†ç±»å™¨
        
        å‚æ•°:
            **params: é€»è¾‘å›å½’çš„è¶…å‚æ•°
        
        è¿”å›:
            LogisticRegression å¯¹è±¡
        """
        lr_params = {
            'max_iter': self.max_iter,
            'random_state': self.random_state,
        }
        solver = params.pop('solver', None)
        if solver is None:
            solver = self.default_params.get('solver')
        if solver is None and self.default_solver:
            solver = self.default_solver
        if solver is None and self.valid_solver_penalty_pairs:
            solver = self.valid_solver_penalty_pairs[0][0]
        if solver is not None:
            lr_params['solver'] = solver
        lr_params.update(params)
        clean_params = {k: v for k, v in lr_params.items() if v is not None}
        model = LogisticRegression()
        if clean_params:
            model.set_params(**clean_params)
        return model

    def _param_grid(self):
        """è¿”å›ç”¨äºæœç´¢çš„å‚æ•°ç½‘æ ¼"""
        param_grid = []
        
        for solver, penalty in self.valid_solver_penalty_pairs:
            if penalty == 'elasticnet':
                grid = {
                    "C": self.C_values,
                    "penalty": [penalty],
                    "solver": [solver],
                    "l1_ratio": self.l1_ratios
                }
            else:
                grid = {
                    "C": self.C_values,
                    "penalty": [penalty],
                    "solver": [solver]
                }
            param_grid.append(grid)
        
        return param_grid

    def build_model(self):
        """æ„å»ºé»˜è®¤çš„é€»è¾‘å›å½’åˆ†ç±»å™¨"""
        return self._build_model()

    def build_model_with_params(self, **params):
        """æ„å»ºæŒ‡å®šè¶…å‚æ•°çš„é€»è¾‘å›å½’åˆ†ç±»å™¨
        
        å‚æ•°:
            **params: é€»è¾‘å›å½’çš„è¶…å‚æ•°
            
        è¿”å›:
            é…ç½®å¥½çš„ LogisticRegression å¯¹è±¡
        """
        return self._build_model(**params)

    def fit(self, X_train, y_train, feature_names=None, show_progress=False):
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹å¹¶è¿›è¡Œè¶…å‚æ•°æœç´¢
        
        æ ¹æ®é€‰æ‹©çš„æœç´¢æ–¹æ³•æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆã€‚
        è®­ç»ƒå®Œæˆåï¼Œæœ€ä½³æ¨¡å‹ä¿å­˜åœ¨self.best_modelä¸­ã€‚
        
        å‚æ•°:
            X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)ï¼ˆå·²é¢„å¤„ç†ï¼‰
            y_train: è®­ç»ƒæ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆç”¨äºç³»æ•°åˆ†æï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„è®­ç»ƒè¿›åº¦
        """
        logger = self._get_logger()
        if self.do_search:
            logger.info("ğŸ“Š å¼€å§‹é€»è¾‘å›å½’è®­ç»ƒï¼Œä½¿ç”¨ç½‘æ ¼æœç´¢...")
        else:
            logger.info("â­ï¸ è·³è¿‡è¶…å‚æ•°æœç´¢ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘å›å½’é…ç½®è¿›è¡Œè®­ç»ƒ")
        
        self.feature_names = feature_names
        model = self._build_model()
        if not self.do_search:
            try:
                model.set_params(**{k: v for k, v in self.default_params.items() if v is not None})
            except Exception:
                pass
            model.fit(X_train, y_train)
            self.best_model = model
            self.cv_results_ = None
            logger.info("âœ… è®­ç»ƒå®Œæˆï¼(æœªè¿›è¡Œæœç´¢)")
        else:
            param_grid = self._param_grid()
            search = GridSearchCV(
                model,
                param_grid=param_grid,
                scoring=self.scoring,
                cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                n_jobs=self.n_jobs,
                verbose=1 if show_progress else 0,
                error_score='raise',  # type: ignore[arg-type]
            )
            search.fit(X_train, y_train)
            self.best_model = search.best_estimator_
            self.cv_results_ = search.cv_results_
            logger.log_cv_results(search)
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³CVå‡†ç¡®ç‡: {search.best_score_:.4f}")
            logger.info(f"ğŸ“‹ æœ€ä½³å‚æ•°: {search.best_params_}")
        
        # åˆ†æç‰¹å¾ç³»æ•°
        self._analyze_coefficients()

    def _analyze_coefficients(self):
        """åˆ†æç‰¹å¾ç³»æ•°"""
        if self.best_model is None:
            return
        
        # è·å–é€»è¾‘å›å½’åˆ†ç±»å™¨
        coefficients = self.best_model.coef_[0]
        
        logger = self._get_logger()
        logger.info(f"\nğŸ“ˆ æ¨¡å‹ç³»æ•°åˆ†æ:")
        intercept_value = np.ravel(getattr(self.best_model, "intercept_", np.array([0.0])))[0]
        logger.info(f"   æˆªè·é¡¹: {intercept_value:.4f}")
        logger.info(f"   ç‰¹å¾ç³»æ•°èŒƒå›´: [{coefficients.min():.4f}, {coefficients.max():.4f}]")
        logger.info(f"   éé›¶ç³»æ•°æ•°é‡: {np.count_nonzero(coefficients)}/{len(coefficients)}")
        
        # æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾
        if self.feature_names is not None and len(self.feature_names) == len(coefficients):
            abs_coef = np.abs(coefficients)
            top_indices = np.argsort(abs_coef)[-10:][::-1]
            
            logger.info(f"\nğŸ” å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
            for i, idx in enumerate(top_indices):
                logger.info(f"   {i+1:2d}. {self.feature_names[idx]}: {coefficients[idx]:.4f}")

    def evaluate(self, X, y, name):
        """åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)
            y: çœŸå®æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)
            name: æ•°æ®é›†åç§°ï¼ˆå¦‚"Validation", "Test"ï¼‰ï¼Œç”¨äºæ‰“å°ä¿¡æ¯
            
        è¿”å›:
            dict: ä¸»è¦åˆ†ç±»æŒ‡æ ‡ï¼ˆaccuracy/precision/recall/f1/aucï¼‰
        """
        if self.best_model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•ã€‚")
        
        # è¿›è¡Œé¢„æµ‹
        y_pred = self.best_model.predict(X)
        y_pred_proba = self.best_model.predict_proba(X)[:, 1]
        logger = self._get_logger()
        metrics_dict = compute_classification_metrics(y, y_pred, y_proba=y_pred_proba, positive_label=1)
        parts = [
            "acc={:.4f}".format(metrics_dict['accuracy']),
            "prec={:.4f}".format(metrics_dict['precision']),
            "rec={:.4f}".format(metrics_dict['recall']),
            "f1={:.4f}".format(metrics_dict['f1']),
        ]
        if 'auc' in metrics_dict:
            parts.append("auc={:.4f}".format(metrics_dict['auc']))
        msg = "{} | {}".format(name, ", ".join(parts))
        logger.info(msg)
        return metrics_dict

    def get_feature_importance(self, top_k=10):
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºç³»æ•°ç»å¯¹å€¼ï¼‰
        
        å‚æ•°:
            top_k: è¿”å›å‰kä¸ªæœ€é‡è¦çš„ç‰¹å¾
            
        è¿”å›:
            ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
        """
        if self.best_model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•ã€‚")
        
        coefficients = self.best_model.coef_[0]
        abs_coef = np.abs(coefficients)
        
        # è·å–é‡è¦æ€§ç´¢å¼•æ’åº
        importance_indices = np.argsort(abs_coef)[::-1][:top_k]
        importance_values = abs_coef[importance_indices]
        
        logger = self._get_logger()
        logger.info(f"\nğŸ” å‰{top_k}ä¸ªæœ€é‡è¦ç‰¹å¾ï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼ï¼‰:")
        for i, (idx, importance) in enumerate(zip(importance_indices, importance_values)):
            coef_sign = "+" if coefficients[idx] >= 0 else "-"
            feature_name = self.feature_names[idx] if self.feature_names else f"ç‰¹å¾{idx}"
            logger.info(f"{i+1:2d}. {feature_name}: {coef_sign}{importance:.4f}")
        
        return importance_indices, importance_values, coefficients[importance_indices]

    def predict_with_confidence(self, X, confidence_threshold=0.7):
        """å¸¦ç½®ä¿¡åº¦çš„é¢„æµ‹
        
        å‚æ•°:
            X: è¾“å…¥ç‰¹å¾
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        è¿”å›:
            é¢„æµ‹ç»“æœã€æ¦‚ç‡å’Œç½®ä¿¡åº¦æ ‡è®°
        """
        if self.best_model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•ã€‚")
        
        y_pred = self.best_model.predict(X)
        y_pred_proba = self.best_model.predict_proba(X)
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆæœ€å¤§æ¦‚ç‡ï¼‰
        max_proba = np.max(y_pred_proba, axis=1)
        high_confidence = max_proba >= confidence_threshold
        
        results = {
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'confidence': max_proba,
            'high_confidence': high_confidence,
            'high_confidence_ratio': np.mean(high_confidence)
        }
        
        logger = self._get_logger()
        logger.info(f"ğŸ”® é¢„æµ‹å®Œæˆ:")
        logger.info(f"   é«˜ç½®ä¿¡åº¦æ ·æœ¬æ¯”ä¾‹: {results['high_confidence_ratio']:.2%}")
        logger.info(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(max_proba):.4f}")
        
        return results

    def save(self, save_path, save_results=None, config=None):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ç£ç›˜
        
        ä½¿ç”¨joblibä¿å­˜Pipelineï¼ˆåŒ…æ‹¬é¢„å¤„ç†æ­¥éª¤å’Œåˆ†ç±»å™¨ï¼‰ï¼Œå¹¶å¯é€‰ä¿å­˜è®­ç»ƒç»“æœä¸é…ç½®åˆ°JSONã€‚
        
        å‚æ•°:
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆ.joblibæˆ–.pklæ–‡ä»¶ï¼‰
            save_results: è®­ç»ƒ/è¯„ä¼°ç»“æœå­—å…¸ï¼ˆå¯é€‰ï¼‰
            config: è®­ç»ƒé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        if self.best_model is None:
            raise RuntimeError("æ²¡æœ‰æ¨¡å‹å¯ä¿å­˜ã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        logger = self._get_logger()
        save_sklearn_model(
            self.best_model,
            save_path,
            model_type='LogisticRegression',
            save_results=save_results,
            config=config,
            extra_model_info={
                'solver': getattr(self.best_model, 'solver', self.default_solver),
                'preprocessing': 'ç‰¹å¾å·²é¢„å¤„ç†ï¼ˆåœ¨ç‰¹å¾æå–é˜¶æ®µå®Œæˆï¼‰'
            },
            logger=logger
        )
