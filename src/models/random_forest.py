"""
çŒ«ç‹—åˆ†ç±» - éšæœºæ£®æ—æ¨¡å‹

è¯¥æ¨¡å—å®ç°äº†åŸºäºscikit-learnçš„éšæœºæ£®æ—åˆ†ç±»å™¨ã€‚
éšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªå†³ç­–æ ‘æ¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚

ä¸»è¦åŠŸèƒ½:
    - è‡ªåŠ¨è¶…å‚æ•°æœç´¢ï¼ˆç½‘æ ¼æœç´¢ï¼‰
    - ç‰¹å¾é‡è¦æ€§åˆ†æ
    - å®Œæ•´çš„æ¨¡å‹è¯„ä¼°å’Œä¿å­˜åŠŸèƒ½
"""
import numpy as np

from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from src.utils.ml_training import compute_classification_metrics, save_sklearn_model
from src.utils.logger import Logger


class RandomForestTrainer:
    """éšæœºæ£®æ—è®­ç»ƒå™¨ç±»
    
    å°è£…äº†éšæœºæ£®æ—çš„è®­ç»ƒã€è¶…å‚æ•°æœç´¢ã€è¯„ä¼°å’Œä¿å­˜åŠŸèƒ½ã€‚
    éšæœºæ£®æ—é€šè¿‡Bootstrapèšåˆå’Œç‰¹å¾éšæœºé€‰æ‹©æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚
    
    å‚æ•°:
        n_estimators_values: å†³ç­–æ ‘æ•°é‡çš„å€™é€‰å€¼åˆ—è¡¨
        max_depth_values: æ ‘æœ€å¤§æ·±åº¦çš„å€™é€‰å€¼åˆ—è¡¨
        min_samples_split_values: åˆ†è£‚å†…éƒ¨èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°å€™é€‰å€¼
        min_samples_leaf_values: å¶å­èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°å€™é€‰å€¼
        max_features_values: å¯»æ‰¾æœ€ä½³åˆ†å‰²æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°é‡ï¼ˆ'sqrt', 'log2', Noneç­‰ï¼‰
        cv_folds: äº¤å‰éªŒè¯çš„æŠ˜æ•°
        n_jobs: å¹¶è¡Œè®­ç»ƒçš„ä½œä¸šæ•°ï¼ˆ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼‰
        random_state: éšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°
        max_samples: Bootstrapé‡‡æ ·æ—¶ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•°ï¼ˆfloatè¡¨ç¤ºæ¯”ä¾‹ï¼Œintè¡¨ç¤ºç»å¯¹æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
    """
    
    def __init__(
        self,
        n_estimators_values=[100, 200, 300],
        max_depth_values=[10, 20, 30, None],
        min_samples_split_values=[2, 5, 10],
        min_samples_leaf_values=[1, 2, 4],
        max_features_values=['sqrt', 'log2'],
        cv_folds=5,
        n_jobs=4,
        random_state=42,
        scoring='accuracy',
        do_search=True,
        default_params=None,
        max_samples=None,
    ):
        self.n_estimators_values = n_estimators_values
        self.max_depth_values = max_depth_values
        self.min_samples_split_values = min_samples_split_values
        self.min_samples_leaf_values = min_samples_leaf_values
        self.max_features_values = max_features_values
        self.cv_folds = cv_folds
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.best_model = None
        self.feature_importance_ = None
        self.scoring = scoring
        self.do_search = do_search
        self.logger = None
        self.default_params = default_params or {}
        self.cv_results_ = None
        self.max_samples = max_samples
    
    def _get_logger(self):
        """è·å–loggerï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„"""
        if self.logger is None:
            self.logger = Logger(name="random_forest_trainer")
        return self.logger

    def _build_model(self, **params):
        """æ„å»ºéšæœºæ£®æ—åˆ†ç±»å™¨
        
        å‚æ•°:
            **params: éšæœºæ£®æ—çš„è¶…å‚æ•°
        
        è¿”å›:
            RandomForestClassifier å¯¹è±¡
        """
        model_params = {
            "random_state": self.random_state,
            "n_jobs": self.n_jobs,
        }
        # å¦‚æœè®¾ç½®äº† max_samplesï¼Œæ·»åŠ åˆ°æ¨¡å‹å‚æ•°ä¸­
        if self.max_samples is not None:
            model_params["max_samples"] = self.max_samples
        return RandomForestClassifier(**model_params)

    def _param_grid(self):
        """è¿”å›ç”¨äºæœç´¢çš„å‚æ•°ç½‘æ ¼"""
        param_grid = {
            "n_estimators": self.n_estimators_values,
            "max_depth": self.max_depth_values,
            "min_samples_split": self.min_samples_split_values,
            "min_samples_leaf": self.min_samples_leaf_values,
        }
        # æ·»åŠ  max_features åˆ°å‚æ•°ç½‘æ ¼ï¼ˆå¦‚æœæä¾›äº†è¯¥å‚æ•°ï¼‰
        if self.max_features_values:
            param_grid["max_features"] = self.max_features_values
        # å¦‚æœè®¾ç½®äº† max_samplesï¼Œä¹Ÿæ·»åŠ åˆ°å‚æ•°ç½‘æ ¼ä¸­
        if self.max_samples is not None:
            param_grid["max_samples"] = [self.max_samples]
        return param_grid

    def build_model(self):
        """æ„å»ºé»˜è®¤çš„éšæœºæ£®æ—åˆ†ç±»å™¨"""
        return self._build_model()

    def build_model_with_params(self, **params):
        """æ„å»ºæŒ‡å®šè¶…å‚æ•°çš„éšæœºæ£®æ—åˆ†ç±»å™¨
        
        å‚æ•°:
            **params: éšæœºæ£®æ—çš„è¶…å‚æ•°
            
        è¿”å›:
            é…ç½®å¥½çš„ RandomForestClassifier å¯¹è±¡
        """
        model = self._build_model()
        try:
            model.set_params(**params)
        except Exception:
            pass
        return model

    def fit(self, X_train, y_train, show_progress=False):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹å¹¶è¿›è¡Œè¶…å‚æ•°æœç´¢
        
        ä½¿ç”¨ç½‘æ ¼æœç´¢æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆã€‚
        è®­ç»ƒå®Œæˆåï¼Œæœ€ä½³æ¨¡å‹ä¿å­˜åœ¨self.best_modelä¸­ã€‚
        
        å‚æ•°:
            X_train: è®­ç»ƒç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)ï¼ˆå·²é¢„å¤„ç†ï¼‰
            y_train: è®­ç»ƒæ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„è®­ç»ƒè¿›åº¦
        """
        logger = self._get_logger()
        if self.do_search:
            logger.info("ğŸŒ² å¼€å§‹éšæœºæ£®æ—è®­ç»ƒï¼Œä½¿ç”¨ç½‘æ ¼æœç´¢...")
        else:
            logger.info("â­ï¸ è·³è¿‡è¶…å‚æ•°æœç´¢ï¼Œä½¿ç”¨é»˜è®¤éšæœºæ£®æ—é…ç½®è¿›è¡Œè®­ç»ƒ")
        
        model = self._build_model()
        if not self.do_search:
            try:
                model.set_params(**{k: v for k, v in self.default_params.items() if v is not None})
            except Exception:
                pass
            model.fit(X_train, y_train)
            self.best_model = model
            self.cv_results_ = None
        else:
            param_grid = self._param_grid()
            search = GridSearchCV(
                model,
                param_grid=param_grid,
                scoring=self.scoring,
                cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                n_jobs=self.n_jobs,
                verbose=1 if show_progress else 0,
            )
            search.fit(X_train, y_train)
            self.best_model = search.best_estimator_
            self.cv_results_ = search.cv_results_
        
        if hasattr(self.best_model, 'feature_importances_'):
            self.feature_importance_ = self.best_model.feature_importances_
        
        if self.do_search:
            logger.log_cv_results(search)
            logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³CVå‡†ç¡®ç‡: {search.best_score_:.4f}")
            logger.info(f"ğŸ“Š æœ€ä½³å‚æ•°: {search.best_params_}")
        else:
            logger.info("âœ… è®­ç»ƒå®Œæˆï¼(æœªè¿›è¡Œæœç´¢)")

    def evaluate(self, X, y, name):
        """åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        å‚æ•°:
            X: ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º(n_samples, n_features)
            y: çœŸå®æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)
            name: æ•°æ®é›†åç§°ï¼ˆå¦‚"Validation", "Test"ï¼‰ï¼Œç”¨äºæ‰“å°ä¿¡æ¯
            
        è¿”å›:
            dict: ä¸»è¦åˆ†ç±»æŒ‡æ ‡ï¼ˆaccuracy/precision/recall/f1ï¼‰
        """
        if self.best_model is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨fit()æ–¹æ³•ã€‚")
        
        # è¿›è¡Œé¢„æµ‹
        y_pred = self.best_model.predict(X)
        y_proba = None
        if hasattr(self.best_model, 'predict_proba'):
            try:
                y_proba = self.best_model.predict_proba(X)
            except Exception:
                y_proba = None
        logger = self._get_logger()
        metrics_dict = compute_classification_metrics(y, y_pred, y_proba=y_proba, positive_label=1)
        msg = "{} | acc={:.4f}, prec={:.4f}, rec={:.4f}, f1={:.4f}".format(
            name, metrics_dict['accuracy'], metrics_dict['precision'], metrics_dict['recall'], metrics_dict['f1']
        )
        logger.info(msg)
        return metrics_dict

    def get_feature_importance(self, top_k=10):
        """è·å–ç‰¹å¾é‡è¦æ€§æ’åº
        
        å‚æ•°:
            top_k: è¿”å›å‰kä¸ªæœ€é‡è¦çš„ç‰¹å¾
            
        è¿”å›:
            ç‰¹å¾é‡è¦æ€§æ•°ç»„ï¼ˆæŒ‰é‡è¦æ€§é™åºæ’åˆ—ï¼‰
        """
        if self.feature_importance_ is None:
            raise RuntimeError("ç‰¹å¾é‡è¦æ€§ä¸å¯ç”¨ã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        # è·å–é‡è¦æ€§ç´¢å¼•æ’åº
        importance_indices = np.argsort(self.feature_importance_)[::-1][:top_k]
        importance_values = self.feature_importance_[importance_indices]
        
        logger = self._get_logger()
        logger.info(f"\nğŸ” å‰{top_k}ä¸ªæœ€é‡è¦ç‰¹å¾:")
        for i, (idx, importance) in enumerate(zip(importance_indices, importance_values)):
            logger.info(f"{i+1:2d}. ç‰¹å¾{idx:4d}: {importance:.4f}")
        
        return importance_indices, importance_values

    def save(self, save_path, save_results=None, config=None):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°ç£ç›˜
        
        ä½¿ç”¨joblibä¿å­˜æ•´ä¸ªPipelineï¼ˆåŒ…æ‹¬é¢„å¤„ç†æ­¥éª¤å’Œåˆ†ç±»å™¨ï¼‰ï¼Œå¹¶å¯é€‰ä¿å­˜è®­ç»ƒç»“æœä¸é…ç½®ã€‚
        
        å‚æ•°:
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆ.joblibæˆ–.pklæ–‡ä»¶ï¼‰
            save_results: è®­ç»ƒ/è¯„ä¼°ç»“æœå­—å…¸ï¼ˆå¯é€‰ï¼‰
            config: è®­ç»ƒé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        if self.best_model is None:
            raise RuntimeError("æ²¡æœ‰æ¨¡å‹å¯ä¿å­˜ã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚")
        
        logger = self._get_logger()
        save_sklearn_model(
            self.best_model,
            save_path,
            model_type='RandomForest',
            save_results=save_results,
            config=config,
            extra_model_info={
                'preprocessing': 'ç‰¹å¾å·²é¢„å¤„ç†ï¼ˆåœ¨ç‰¹å¾æå–é˜¶æ®µå®Œæˆï¼‰'
            },
            logger=logger
        )
