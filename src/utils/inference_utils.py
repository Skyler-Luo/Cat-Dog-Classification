"""æ¨ç†è„šæœ¬é€šç”¨å·¥å…·å‡½æ•°

è¯¥æ¨¡å—æä¾›æ¨ç†è„šæœ¬ä¸­å¸¸ç”¨çš„å·¥å…·å‡½æ•°ï¼ŒåŒ…æ‹¬ï¼š
- è®­ç»ƒé…ç½®åŠ è½½
- å›¾åƒè·¯å¾„æ”¶é›†
- é¢„æµ‹ç»“æœä¿å­˜
- ç‰¹å¾æå–è¾…åŠ©å‡½æ•°
"""

import json
import csv
from pathlib import Path


def load_training_config(weights_path):
    """åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„é…ç½®æ–‡ä»¶
    
    å‚æ•°:
        weights_path: æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆstr æˆ– Pathï¼‰
        
    è¿”å›:
        dict æˆ– None: è®­ç»ƒé…ç½®å­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥åˆ™è¿”å› None
        
    è¯´æ˜:
        è¯¥å‡½æ•°ä¼šåœ¨æ¨¡å‹æƒé‡åŒç›®å½•ä¸‹æŸ¥æ‰¾ 'training_results.json' æ–‡ä»¶ï¼Œ
        å¹¶ä»ä¸­æå–é…ç½®ä¿¡æ¯ã€‚
        
    ç¤ºä¾‹:
        >>> config = load_training_config('runs/sklearn_svm/best.joblib')
        >>> if config:
        ...     print(config['svm']['C'])
    """
    try:
        cfg_path = Path(weights_path).parent / "training_results.json"
        
        if not cfg_path.exists():
            return None
        
        with open(cfg_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # å…¼å®¹ä¸åŒçš„é…ç½®æ ¼å¼
        return data.get("config") or data
        
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½è®­ç»ƒé…ç½®: {e}")
        return None


def gather_image_paths(path, extensions=None, recursive=True, verbose=True):
    """æ”¶é›†å¾…æ¨ç†çš„å›¾ç‰‡è·¯å¾„
    
    å‚æ•°:
        path: æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„ï¼ˆstr æˆ– Pathï¼‰
        extensions: æ”¯æŒçš„æ‰©å±•åé›†åˆï¼ˆset æˆ– Noneï¼‰
                   é»˜è®¤: {'.jpg', '.jpeg', '.png', '.bmp'}
        recursive: æ˜¯å¦é€’å½’æœç´¢å­ç›®å½•ï¼ˆboolï¼Œé»˜è®¤: Trueï¼‰
        verbose: æ˜¯å¦æ‰“å°æ”¶é›†ä¿¡æ¯ï¼ˆboolï¼Œé»˜è®¤: Trueï¼‰
        
    è¿”å›:
        list[str]: å›¾ç‰‡è·¯å¾„åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        
    è¯´æ˜:
        - å¦‚æœ path æ˜¯æ–‡ä»¶ï¼Œæ£€æŸ¥æ‰©å±•åå¹¶è¿”å›å•å…ƒç´ åˆ—è¡¨
        - å¦‚æœ path æ˜¯ç›®å½•ï¼Œæœç´¢æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å›¾ç‰‡
        - æ”¯æŒé€’å½’å’Œéé€’å½’ä¸¤ç§æœç´¢æ¨¡å¼
        
    ç¤ºä¾‹:
        >>> # æ”¶é›†å•ä¸ªæ–‡ä»¶
        >>> paths = gather_image_paths('test.jpg')
        ['test.jpg']
        
        >>> # æ”¶é›†ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡
        >>> paths = gather_image_paths('dataset/test')
        ğŸ“¸ æ”¶é›†å›¾ç‰‡: dataset/test
           â€¢ æ‰¾åˆ° 1000 å¼ å›¾ç‰‡
        
        >>> # ä»…æ”¶é›† jpg å’Œ png
        >>> paths = gather_image_paths('dataset', extensions={'.jpg', '.png'})
    """
    if extensions is None:
        extensions = {".jpg", ".jpeg", ".png", ".bmp"}
    
    # ç¡®ä¿æ‰©å±•åä¸ºå°å†™
    extensions = {ext.lower() for ext in extensions}
    
    p = Path(path)
    
    # å¤„ç†å•ä¸ªæ–‡ä»¶
    if p.is_file():
        if p.suffix.lower() in extensions:
            if verbose:
                print(f"ğŸ“¸ å•ä¸ªæ–‡ä»¶: {p}")
            return [str(p)]
        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å: {p.suffix}ï¼ˆæ”¯æŒ: {extensions}ï¼‰")
            return []
    
    # å¤„ç†ç›®å½•
    if p.is_dir():
        if verbose:
            print(f"ğŸ“¸ æ”¶é›†å›¾ç‰‡: {p}")
        
        paths = []
        
        if recursive:
            # é€’å½’æœç´¢
            for ext in extensions:
                paths.extend([str(q) for q in p.rglob(f"*{ext}")])
        else:
            # ä»…æœç´¢å½“å‰ç›®å½•
            for ext in extensions:
                paths.extend([str(q) for q in p.glob(f"*{ext}")])
        
        paths = sorted(paths)
        
        if verbose:
            print(f"   â€¢ æ‰¾åˆ° {len(paths)} å¼ å›¾ç‰‡")
        
        return paths
    
    # è·¯å¾„ä¸å­˜åœ¨
    print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
    return []


def save_predictions_to_csv(results, csv_path, verbose=True):
    """ä¿å­˜é¢„æµ‹ç»“æœåˆ° CSV æ–‡ä»¶
    
    å‚æ•°:
        results: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯é¡¹ä¸º (image_path, prediction, confidence) å…ƒç»„
        csv_path: CSV æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆstr æˆ– Pathï¼‰
        verbose: æ˜¯å¦æ‰“å°ä¿å­˜ä¿¡æ¯ï¼ˆboolï¼Œé»˜è®¤: Trueï¼‰
        
    è¯´æ˜:
        CSV æ–‡ä»¶æ ¼å¼ï¼š
        - ç¬¬ä¸€è¡Œï¼šåˆ—æ ‡é¢˜ï¼ˆImage, Prediction, Confidenceï¼‰
        - åç»­è¡Œï¼šå›¾åƒè·¯å¾„ã€é¢„æµ‹ç±»åˆ«ã€ç½®ä¿¡åº¦
        
    ç¤ºä¾‹:
        >>> results = [
        ...     ('img1.jpg', 'cat', 0.95),
        ...     ('img2.jpg', 'dog', 0.88),
        ... ]
        >>> save_predictions_to_csv(results, 'predictions.csv')
        ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: predictions.csv
           â€¢ å…± 2 æ¡è®°å½•
    """
    csv_path = Path(csv_path)
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # å†™å…¥è¡¨å¤´
        writer.writerow(['Image', 'Prediction', 'Confidence'])
        
        # å†™å…¥æ•°æ®
        for img_path, pred, conf in results:
            writer.writerow([img_path, pred, f'{conf:.4f}'])
    
    if verbose:
        print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {csv_path}")
        print(f"   â€¢ å…± {len(results)} æ¡è®°å½•")


def print_predictions(results, max_display=10):
    """æ‰“å°é¢„æµ‹ç»“æœåˆ°æ§åˆ¶å°
    
    å‚æ•°:
        results: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯é¡¹ä¸º (image_path, prediction, confidence) å…ƒç»„
        max_display: æœ€å¤šæ˜¾ç¤ºçš„è®°å½•æ•°ï¼ˆintï¼Œé»˜è®¤: 10ï¼‰
        
    ç¤ºä¾‹:
        >>> results = [('img1.jpg', 'cat', 0.95), ('img2.jpg', 'dog', 0.88)]
        >>> print_predictions(results)
        
        ğŸ“Š é¢„æµ‹ç»“æœ:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. img1.jpg
           é¢„æµ‹: ğŸ± cat (ç½®ä¿¡åº¦: 95.00%)
        2. img2.jpg
           é¢„æµ‹: ğŸ¶ dog (ç½®ä¿¡åº¦: 88.00%)
    """
    if not results:
        print("âŒ æ²¡æœ‰é¢„æµ‹ç»“æœ")
        return
    
    print(f"\nğŸ“Š é¢„æµ‹ç»“æœ:")
    print("â”€" * 60)
    
    # æ˜¾ç¤ºå‰ max_display æ¡
    for i, (img_path, pred, conf) in enumerate(results[:max_display], 1):
        # è·å–æ–‡ä»¶å
        filename = Path(img_path).name
        
        # é€‰æ‹©è¡¨æƒ…ç¬¦å·
        emoji = "ğŸ±" if pred.lower() in ['cat', '0', 'cats'] else "ğŸ¶"
        
        print(f"{i}. {filename}")
        print(f"   é¢„æµ‹: {emoji} {pred} (ç½®ä¿¡åº¦: {conf*100:.2f}%)")
    
    # å¦‚æœæœ‰æ›´å¤šç»“æœï¼Œæ˜¾ç¤ºçœç•¥ä¿¡æ¯
    if len(results) > max_display:
        print(f"\n... è¿˜æœ‰ {len(results) - max_display} æ¡è®°å½•ï¼ˆçœç•¥æ˜¾ç¤ºï¼‰")
    
    print("â”€" * 60)
    print(f"æ€»è®¡: {len(results)} å¼ å›¾ç‰‡")


def format_prediction_summary(results):
    """æ ¼å¼åŒ–é¢„æµ‹ç»“æœæ‘˜è¦
    
    å‚æ•°:
        results: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯é¡¹ä¸º (image_path, prediction, confidence) å…ƒç»„
        
    è¿”å›:
        dict: æ‘˜è¦ç»Ÿè®¡ï¼ŒåŒ…å«ï¼š
            - total: æ€»é¢„æµ‹æ•°é‡
            - cat_count: é¢„æµ‹ä¸ºçŒ«çš„æ•°é‡
            - dog_count: é¢„æµ‹ä¸ºç‹—çš„æ•°é‡
            - avg_confidence: å¹³å‡ç½®ä¿¡åº¦
            - high_confidence: é«˜ç½®ä¿¡åº¦ï¼ˆ>0.9ï¼‰æ ·æœ¬æ•°
            - low_confidence: ä½ç½®ä¿¡åº¦ï¼ˆ<0.6ï¼‰æ ·æœ¬æ•°
            
    ç¤ºä¾‹:
        >>> summary = format_prediction_summary(results)
        >>> print(summary['cat_count'])
        45
    """
    if not results:
        return {
            'total': 0,
            'cat_count': 0,
            'dog_count': 0,
            'avg_confidence': 0.0,
            'high_confidence': 0,
            'low_confidence': 0
        }
    
    cat_count = 0
    dog_count = 0
    confidences = []
    
    for _, pred, conf in results:
        confidences.append(conf)
        
        if pred.lower() in ['cat', '0', 'cats']:
            cat_count += 1
        else:
            dog_count += 1
    
    avg_conf = sum(confidences) / len(confidences)
    high_conf = sum(1 for c in confidences if c > 0.9)
    low_conf = sum(1 for c in confidences if c < 0.6)
    
    return {
        'total': len(results),
        'cat_count': cat_count,
        'dog_count': dog_count,
        'avg_confidence': avg_conf,
        'high_confidence': high_conf,
        'low_confidence': low_conf
    }


def print_prediction_summary(summary):
    """æ‰“å°é¢„æµ‹ç»“æœæ‘˜è¦
    
    å‚æ•°:
        summary: ç”± format_prediction_summary() è¿”å›çš„æ‘˜è¦å­—å…¸
        
    ç¤ºä¾‹:
        >>> summary = format_prediction_summary(results)
        >>> print_prediction_summary(summary)
        
        ğŸ“ˆ é¢„æµ‹æ‘˜è¦:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        æ€»æ•°é‡: 100
        ğŸ± çŒ«: 45 (45.0%)
        ğŸ¶ ç‹—: 55 (55.0%)
        å¹³å‡ç½®ä¿¡åº¦: 87.5%
        é«˜ç½®ä¿¡åº¦ (>90%): 60
        ä½ç½®ä¿¡åº¦ (<60%): 5
    """
    total = summary['total']
    
    if total == 0:
        print("âŒ æ²¡æœ‰é¢„æµ‹ç»“æœ")
        return
    
    print(f"\nğŸ“ˆ é¢„æµ‹æ‘˜è¦:")
    print("â”€" * 40)
    print(f"æ€»æ•°é‡: {total}")
    print(f"ğŸ± çŒ«: {summary['cat_count']} ({summary['cat_count']/total*100:.1f}%)")
    print(f"ğŸ¶ ç‹—: {summary['dog_count']} ({summary['dog_count']/total*100:.1f}%)")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {summary['avg_confidence']*100:.1f}%")
    print(f"é«˜ç½®ä¿¡åº¦ (>90%): {summary['high_confidence']}")
    print(f"ä½ç½®ä¿¡åº¦ (<60%): {summary['low_confidence']}")


def create_feature_extractor_from_config(config, fallback_preset="balanced", fallback_size=64, n_jobs=8):
    """ä»è®­ç»ƒé…ç½®åˆ›å»ºç‰¹å¾æå–å™¨
    
    å‚æ•°:
        config: è®­ç»ƒé…ç½®å­—å…¸ï¼ˆå¯ä¸ºNoneï¼‰
        fallback_preset: é»˜è®¤é¢„è®¾ï¼ˆstrï¼Œé»˜è®¤: 'balanced'ï¼‰
        fallback_size: é»˜è®¤å›¾åƒå°ºå¯¸ï¼ˆintï¼Œé»˜è®¤: 64ï¼‰
        n_jobs: å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆintï¼Œé»˜è®¤: 8ï¼‰
        
    è¿”å›:
        UnifiedFeatureExtractorå®ä¾‹
        
    è¯´æ˜:
        è¯¥å‡½æ•°å°è¯•ä»è®­ç»ƒé…ç½®ä¸­è¯»å–ç‰¹å¾æå–å‚æ•°ï¼Œå¦‚æœé…ç½®ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥ï¼Œ
        åˆ™ä½¿ç”¨fallbackå‚æ•°ã€‚è¿™ç¡®ä¿äº†æ¨ç†æ—¶çš„ç‰¹å¾æå–ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ã€‚
        
    ç¤ºä¾‹:
        >>> config = load_training_config('runs/sklearn_svm/best.joblib')
        >>> extractor = create_feature_extractor_from_config(config)
        ğŸ“‹ ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾é…ç½®
        ğŸ§© ç‰¹å¾é…ç½®: preset=balanced, image_size=64
    """
    from src.data.feature_extract import UnifiedFeatureExtractor
    
    # ä»é…ç½®ä¸­æå–å‚æ•°
    if config and isinstance(config, dict):
        feat_cfg = config.get("features", {})
        preset = feat_cfg.get("preset", fallback_preset)
        image_size = feat_cfg.get("image_size", fallback_size)
        enable_preproc = bool(feat_cfg.get("enable_extractor_preprocessing", False))
        print(f"ğŸ“‹ ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾é…ç½®")
    else:
        preset = fallback_preset
        image_size = fallback_size
        enable_preproc = False
        print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    
    print(f"ğŸ§© ç‰¹å¾é…ç½®: preset={preset}, image_size={image_size}")
    
    return UnifiedFeatureExtractor(
        feature_config=preset,
        image_size=image_size,
        enable_preprocessing=enable_preproc,
        n_jobs=n_jobs,
        verbose=False
    )


def extract_features_for_inference(image_paths, extractor, show_progress=True):
    """æå–æ¨ç†ç‰¹å¾ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
    
    å‚æ•°:
        image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
        extractor: ç‰¹å¾æå–å™¨å®ä¾‹ï¼ˆUnifiedFeatureExtractorï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆboolï¼Œé»˜è®¤: Trueï¼‰
        
    è¿”å›:
        tuple: (features, valid_paths) - ç‰¹å¾çŸ©é˜µå’Œæœ‰æ•ˆè·¯å¾„åˆ—è¡¨
        
    å¼‚å¸¸:
        RuntimeError: å¦‚æœç‰¹å¾æå–å¤±è´¥æˆ–æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡
        
    ç¤ºä¾‹:
        >>> extractor = UnifiedFeatureExtractor(...)
        >>> features, valid_paths = extract_features_for_inference(image_paths, extractor)
        ğŸ¨ æå–ç‰¹å¾...
        100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:30<00:00, 33.33it/s]
           âœ… æˆåŠŸæå– 1000 ä¸ªæ ·æœ¬çš„ç‰¹å¾
    """
    print(f"\nğŸ¨ æå–ç‰¹å¾...")
    features, _, valid_indices = extractor.extract_features_batch(
        image_paths,
        labels=None,
        show_progress=show_progress
    )
    
    if len(features) == 0:
        raise RuntimeError("âŒ ç‰¹å¾æå–å¤±è´¥ï¼Œæ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡")
    
    valid_paths = [image_paths[i] for i in valid_indices]
    print(f"   âœ… æˆåŠŸæå– {len(features)} ä¸ªæ ·æœ¬çš„ç‰¹å¾")
    
    return features, valid_paths

