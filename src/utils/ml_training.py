"""ä¼ ç»Ÿæœºå™¨å­¦ä¹ è®­ç»ƒé€šç”¨å·¥å…·å‡½æ•°ã€‚"""

import random
import time
import joblib
import numpy as np
from pathlib import Path
from datetime import datetime
from sklearn import metrics

from .config import MODEL_TYPE_NAMES
from .logger import Logger
from tools.visualization import (
    plot_split_metrics,
    plot_confusion_matrix,
    plot_metric_curves,
    plot_roc_curve,
    plot_cv_results,
)


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    except ImportError:
        pass


def load_features(features_dir, split, verbose=True, logger=None):
    features_path = Path(features_dir) / '{}_features.joblib'.format(split)
    if not features_path.exists():
        raise FileNotFoundError(
            'ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {}\nğŸ’¡ æç¤ºï¼šè¯·å…ˆè¿è¡Œ \'python tools/extract_best_features.py\' æå–ç‰¹å¾'.format(features_path)
        )
    Logger.maybe_print(verbose, logger, 'ğŸ“¥ åŠ è½½ç‰¹å¾: {}'.format(features_path))
    data = joblib.load(features_path)
    if isinstance(data, dict):
        X = data.get('features', data.get('X'))
        y = data.get('labels', data.get('y'))
    elif isinstance(data, (tuple, list)) and len(data) >= 2:
        X, y = data[0], data[1]
    else:
        raise ValueError('æ— æ³•è§£æç‰¹å¾æ–‡ä»¶æ ¼å¼: {}\næœŸæœ›æ ¼å¼: dict æˆ– (X, y) å…ƒç»„'.format(features_path))
    if X is None:
        raise ValueError('ç‰¹å¾çŸ©é˜µä¸º None: {}'.format(features_path))
    y = np.array(y) if y is not None else None
    if len(X) == 0:
        raise ValueError('ç‰¹å¾çŸ©é˜µä¸ºç©º: {}'.format(features_path))
    if X.ndim != 2:
        raise ValueError('ç‰¹å¾çŸ©é˜µç»´åº¦é”™è¯¯: æœŸæœ› 2Dï¼Œå®é™… {}Dï¼Œå½¢çŠ¶: {}'.format(X.ndim, X.shape))
    if y is not None and len(X) != len(y):
        raise ValueError('ç‰¹å¾å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…: X={}, y={}'.format(len(X), len(y)))
    Logger.maybe_print(verbose, logger, '   â€¢ ç‰¹å¾å½¢çŠ¶: {}'.format(X.shape))
    if y is not None:
        Logger.maybe_print(verbose, logger, '   â€¢ æ ‡ç­¾æ•°é‡: {}'.format(len(y)))
        Logger.maybe_print(verbose, logger, '   â€¢ ç±»åˆ«: {}'.format(np.unique(y)))
    return X, y


def load_train_val_test(features_dir, train_split='train', val_split='val', test_split='test', verbose=True, logger=None):
    Logger.maybe_print(verbose, logger, 'ğŸ“Š åŠ è½½æ•°æ®é›†...')
    datasets = {}
    for split_name, split_key in [(train_split, 'train'), (val_split, 'val'), (test_split, 'test')]:
        try:
            X, y = load_features(features_dir, split_name, verbose=verbose, logger=logger)
            datasets[split_key] = (X, y)
        except FileNotFoundError:
            Logger.maybe_print(verbose, logger, 'âš ï¸  {} é›†ä¸å­˜åœ¨ï¼Œè·³è¿‡'.format(split_key.upper()))
            datasets[split_key] = (None, None)
    Logger.maybe_print(verbose, logger, 'ğŸ“Š æ•°æ®é›†è§„æ¨¡:')
    for split_key in ['train', 'val', 'test']:
        X, y = datasets[split_key]
        if X is not None:
            msg = '   â€¢ {}: {} æ ·æœ¬'.format(split_key.upper().ljust(5), len(X))
        else:
            msg = '   â€¢ {}: -'.format(split_key.upper().ljust(5))
        Logger.maybe_print(verbose, logger, msg)
    return datasets


def evaluate_all_splits(trainer, datasets, verbose=True, logger=None):
    Logger.maybe_print(verbose, logger, 'ğŸ“Š è¯„ä¼°æ¨¡å‹...')
    results = {}
    for split_name in ['train', 'val', 'test']:
        X, y = datasets.get(split_name, (None, None))
        if X is not None and y is not None:
            results[split_name] = trainer.evaluate(X, y, name=split_name.capitalize())
        else:
            results[split_name] = None
    return results


def build_config(args, model_type, model_specific_config):
    return {
        'data': {
            'features_dir': getattr(args, 'data_dir', 'features/best_features'),
            'train_dirname': getattr(args, 'train_dirname', 'train'),
            'val_dirname': getattr(args, 'val_dirname', 'val'),
            'test_dirname': getattr(args, 'test_dirname', 'test'),
        },
        model_type: model_specific_config,
        'runtime': {
            'n_jobs': getattr(args, 'n_jobs', 8),
            'seed': getattr(args, 'seed', 42),
            'timestamp': datetime.now().isoformat(),
        },
        'preprocessing': 'ç‰¹å¾å·²é¢„å¤„ç†ï¼ˆStandardScaler + PCA å·²åœ¨ç‰¹å¾æå–é˜¶æ®µå®Œæˆï¼‰',
    }


def parse_numeric_or_str(value):
    try:
        return float(value)
    except (ValueError, TypeError):
        return value


def parse_param_list(values):
    return [parse_numeric_or_str(v) for v in values]


def run_sklearn_training(args, trainer_class, trainer_kwargs, model_type, hyperparams_info=None, logger=None):
    set_seed(args.seed)
    start_time = time.time()
    model_name = MODEL_TYPE_NAMES.get(model_type, model_type.upper())
    logger = logger or Logger(name="sklearn_training")
    logger.header(model_name, args)
    if hyperparams_info:
        logger.info('ğŸ§ª è¶…å‚æ•°: {}'.format(hyperparams_info))
    datasets = load_train_val_test(
        args.data_dir,
        args.train_dirname,
        args.val_dirname,
        args.test_dirname,
        logger=logger
    )
    X_train, y_train = datasets['train']
    if X_train is None or len(X_train) == 0:
        raise RuntimeError('è®­ç»ƒé›†ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥')
    logger.info('ğŸ”§ åˆ›å»ºå¹¶è®­ç»ƒ {}...'.format(model_name))
    trainer = trainer_class(**trainer_kwargs)
    try:
        setattr(trainer, 'logger', logger)
    except Exception:
        pass
    trainer.fit(X_train, y_train)
    results = evaluate_all_splits(trainer, datasets, logger=logger)
    run_dir = Path(args.save_path).parent
    model_slug = str(model_type).lower().replace(' ', '_')
    _generate_visualizations(trainer, datasets, results, run_dir, model_name, model_slug, logger)
    save_path = Path(args.save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    config = build_config(args, model_type=model_type, model_specific_config=trainer_kwargs)
    try:
        trainer.save(save_path, save_results=results, config=config, logger=logger)
    except TypeError:
        trainer.save(save_path, save_results=results, config=config)
    elapsed = time.time() - start_time
    logger.summary(save_path, elapsed)
    return trainer, results


def _generate_visualizations(trainer, datasets, results, run_dir, model_name, model_slug, logger):
    figures_dir = Path(run_dir) / 'figures'
    figures_dir.mkdir(parents=True, exist_ok=True)
    try:
        slug = model_slug or model_name.lower()
        has_pr_data = any(
            isinstance(results.get(split), dict) and (
                results[split].get('precision_curve') is not None or
                isinstance(results[split].get('pr_curve'), dict)
            )
            for split in ['train', 'val', 'test']
        )
        if has_pr_data:
            plot_split_metrics(
                results,
                metric='pr',
                title="{} PR æ›²çº¿".format(model_name),
                save_path=figures_dir / "{}_pr_curve.png".format(slug)
            )
    except Exception as exc:
        if logger:
            logger.debug("ç”Ÿæˆ PR æ›²çº¿å¤±è´¥: {}".format(exc))
    history = results.get('history')
    if history:
        try:
            plot_metric_curves(
                history,
                title="{} Metric Curves".format(model_name),
                save_path=figures_dir / "metric_curves.png"
            )
        except Exception as exc:
            if logger:
                logger.debug("ç”Ÿæˆè®­ç»ƒæ›²çº¿å¤±è´¥: {}".format(exc))
    for split in ['train', 'val', 'test']:
        X, y = datasets.get(split, (None, None))
        if X is None or y is None:
            continue
        try:
            y_pred = trainer.best_model.predict(X)
            class_names = [str(cls) for cls in sorted(np.unique(y))]
            cm = metrics.confusion_matrix(y, y_pred)
            plot_confusion_matrix(
                cm,
                class_names=class_names,
                title="{} {} æ··æ·†çŸ©é˜µ".format(model_name, split.capitalize()),
                save_path=figures_dir / "{}_{}_confusion.png".format(slug, split)
            )
        except Exception as exc:
            if logger:
                logger.debug("ç”Ÿæˆ {} æ··æ·†çŸ©é˜µå¤±è´¥: {}".format(split, exc))
        try:
            y_scores = None
            if hasattr(trainer.best_model, 'predict_proba'):
                proba = trainer.best_model.predict_proba(X)
                if proba.ndim == 2:
                    y_scores = proba[:, 1]
                else:
                    y_scores = proba
            elif hasattr(trainer.best_model, 'decision_function'):
                y_scores = trainer.best_model.decision_function(X)
            if y_scores is not None and len(np.unique(y)) == 2:
                plot_roc_curve(
                    y,
                    y_scores,
                    title="{} {} ROC".format(model_name, split.capitalize()),
                    save_path=figures_dir / "{}_{}_roc.png".format(slug, split)
                )
        except Exception as exc:
            if logger:
                logger.debug("ç”Ÿæˆ {} ROC æ›²çº¿å¤±è´¥: {}".format(split, exc))
    cv_results = getattr(trainer, 'cv_results_', None)
    if cv_results:
        try:
            plot_cv_results(
                cv_results,
                title="{} å‚æ•°æœç´¢ç»“æœ".format(model_name),
                save_path=figures_dir / "cv_results.png"
            )
        except Exception as exc:
            if logger:
                logger.debug("ç”Ÿæˆå‚æ•°æœç´¢å¯è§†åŒ–å¤±è´¥: {}".format(exc))


def compute_classification_metrics(y_true, y_pred, y_proba=None, positive_label=1, zero_division='warn'):
    """è®¡ç®—åˆ†ç±»ä»»åŠ¡çš„å¸¸ç”¨æŒ‡æ ‡ã€‚
    
    å‚æ•°:
        y_true: çœŸå®æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)
        y_pred: é¢„æµ‹æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)
        y_proba: é¢„æµ‹ä¸ºæ­£ç±»çš„æ¦‚ç‡æˆ–å†³ç­–å¾—åˆ†ï¼Œå½¢çŠ¶ä¸º(n_samples,) æˆ– (n_samples, 2)ï¼ˆå¯é€‰ï¼‰
        positive_label: æ­£ç±»æ ‡ç­¾ï¼ˆé»˜è®¤: 1ï¼‰
        zero_division: å½“å‡ºç°é™¤é›¶æ—¶çš„å¤„ç†æ–¹å¼ï¼ˆ'warn'/'0'/'1'ï¼Œé»˜è®¤: 'warn'ï¼‰
        
    è¿”å›:
        dict: æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å« accuracyã€precisionã€recallã€f1ã€‚å½“æä¾›æ¦‚ç‡æˆ–å¾—åˆ†æ—¶ï¼Œè¿˜åŒ…å«:
            - auc: ROC æ›²çº¿ä¸‹é¢ç§¯
            - average_precision: å¹³å‡ç²¾ç¡®ç‡ï¼ˆAPï¼‰
            - pr_curve: åŒ…å« precisionã€recallã€thresholds åˆ—è¡¨çš„å­—å…¸
    """
    acc = metrics.accuracy_score(y_true, y_pred)
    precision = metrics.precision_score(y_true, y_pred, pos_label=positive_label, zero_division=zero_division)
    recall = metrics.recall_score(y_true, y_pred, pos_label=positive_label, zero_division=zero_division)
    f1 = metrics.f1_score(y_true, y_pred, pos_label=positive_label, zero_division=zero_division)
    result = {}
    result['accuracy'] = float(acc)
    result['precision'] = float(precision)
    result['recall'] = float(recall)
    result['f1'] = float(f1)
    # AUC ä»…åœ¨æä¾›æ¦‚ç‡æ—¶è®¡ç®—
    if y_proba is not None:
        try:
            if y_proba.ndim == 2 and y_proba.shape[1] == 2:
                pos_scores = y_proba[:, 1]
            else:
                pos_scores = y_proba
            auc = metrics.roc_auc_score(y_true, pos_scores)
            result['auc'] = float(auc)
            precision_curve, recall_curve, thresholds = metrics.precision_recall_curve(
                y_true,
                pos_scores,
                pos_label=positive_label
            )
            average_precision = metrics.average_precision_score(y_true, pos_scores, pos_label=positive_label)
            result['average_precision'] = float(average_precision)
            pr_curve_data = {
                'precision': [float(value) for value in precision_curve],
                'recall': [float(value) for value in recall_curve],
                'thresholds': [float(value) for value in thresholds],
            }
            result['pr_curve'] = pr_curve_data
        except Exception:
            pass
    return result


def save_sklearn_model(model, save_path, model_type, save_results=None, config=None, extra_model_info=None, logger=None):
    """ç»Ÿä¸€çš„ sklearn æ¨¡å‹ä¿å­˜å‡½æ•°ï¼ˆå«ç»“æœä¸é…ç½®ï¼‰ã€‚
    
    å‚æ•°:
        model: å·²è®­ç»ƒçš„ sklearn æ¨¡å‹å¯¹è±¡
        save_path: ä¿å­˜è·¯å¾„ï¼ˆstr æˆ– Pathï¼‰
        model_type: æ¨¡å‹ç±»å‹åç§°ï¼ˆstrï¼‰ï¼Œä¾‹å¦‚ 'SVM'ã€'LogisticRegression'ã€'RandomForest'
        save_results: è®­ç»ƒ/è¯„ä¼°ç»“æœå­—å…¸ï¼ˆå¯é€‰ï¼‰
        config: è®­ç»ƒé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        extra_model_info: é¢å¤–çš„æ¨¡å‹ä¿¡æ¯å­—å…¸ï¼ˆå¯é€‰ï¼‰
    """
    import json
    import joblib
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(model, save_path)
    (logger.info('ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {}'.format(save_path)) if logger else print('ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {}'.format(save_path)))
    if save_results is not None or config is not None:
        results_data = {
            'timestamp': datetime.now().isoformat(),
            'model_info': {
                'model_type': model_type,
            }
        }
        if extra_model_info:
            results_data['model_info'].update(extra_model_info)
        if config is not None:
            results_data['config'] = config
        if save_results is not None:
            results_data['results'] = save_results
        results_path = save_path.parent / 'training_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        (logger.info('ğŸ“ è®­ç»ƒé…ç½®ä¸ç»“æœå·²ä¿å­˜: {}'.format(results_path)) if logger else print('ğŸ“ è®­ç»ƒé…ç½®ä¸ç»“æœå·²ä¿å­˜: {}'.format(results_path)))
